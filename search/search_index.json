{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Neural Networks Optimization and Training Tutorial - Hub","text":"<p>Welcome to the learning guide for the Neural Networks Optimization and Training Tutorial (NNOTT) sandbox. This tutorial series walks through the entire project\u2014why it exists, how it is organized, and how to run meaningful experiments. Use the navigation list below to jump to the relevant module.</p>"},{"location":"#how-to-use-this-tutorial","title":"How to Use This Tutorial","text":"<ol> <li>Start with the Project Tour to understand the repository layout and supporting tools.</li> <li>Study the Implementation Progression (scalar to vectorized to GPU) to see how performance grows from simple loops to accelerated kernels.</li> <li>Follow the Experiment Playbook to run the MLP and CNN scripts, save checkpoints, and compare results.</li> <li>Review the Concept Notes whenever you need a refresher on activations, losses, optimizers, or regularization tricks.</li> <li>Explore the Architecture Gallery to learn how classic and modern CNNs (LeNet through ConvNeXt) are assembled from the provided building blocks.</li> </ol>"},{"location":"#tutorial-modules","title":"Tutorial Modules","text":"Module Description Project Tour Repository structure, shared utilities, datasets, and helper scripts. Implementations &amp; Hardware Scalar vs vectorized code paths, backend switching, and GPU diagnostics. Running Experiments Training commands, checkpoint workflows, quick-start scripts, and lab prompts. Core Concepts Notes on neurons, activations, normalization, losses, optimizers, and convolution tricks. Architecture Gallery Background on each CNN (LeNet, AlexNet, VGG16, ResNet18, EfficientNet-Lite0, ConvNeXt-Tiny) plus dataset considerations. Debug Playbook Troubleshooting recipes for environment, GPU, data, and training issues. Optimization Lab Jupyter walkthrough for tuning learning rate, gradient clipping, Lookahead, and batch sizes. Activation Functions Side-by-side comparison of ReLU, LeakyReLU, SiLU, and GELU with sample plots. Experiment Log Template Markdown template for recording dataset, hyperparameters, results, and observations. Performance Profiles Typical training times and accuracies (CPU vs GPU) for each architecture. Augmentation Playground Central entry point for augmentation CLI flags plus deep dives on geometry, noise/cutout, CutMix, and RandAugment. About This Tutorial Overview of the teaching goals and how students should progress through the codebase. Roadmap Step-by-step learning path that suggests what to read/run at each stage."},{"location":"#scenario-reference-table","title":"Scenario Reference Table","text":"Goal Fastest Way to Try Learn optimizer behavior <code>python scripts/quickstart_vectorized.py --scenario optimizer-compare --plot</code> Explore MLP architecture &amp; schedule toggles <code>python scripts/quickstart_vectorized.py --scenario hidden-sweep --hidden-activations relu,gelu,tanh --dropout 0.2 --batchnorm --lr-schedule cosine --early-stopping</code> Compare CPU vs GPU throughput <code>python scripts/quickstart_convolutional.py --scenario gpu-fast --epochs 1 --lookahead --plot</code> (run once with <code>--gpu</code>, once without) Practice checkpoint save/resume <code>python scripts/quickstart_convolutional.py --scenario resume-demo --save-path checkpoints/demo.npz --plot</code> Swap in a new dataset <code>python scripts/quickstart_scalar.py --scenario dataset-swap --plot --alt-train-images fashion_train_images.npy ...</code> Test gradient clipping or Lookahead <code>python convolutional/main.py resnet18 --epochs 1 --batch-size 64 --gpu --grad-clip 5 --lookahead --lookahead-k 5 --lookahead-alpha 0.5</code> Visualize misclassifications <code>python vectorized/main.py --epochs 2 --batch-size 64 --plot</code> (answer \"y\" when prompted) or <code>python convolutional/main.py baseline --plot --show-misclassified</code> (confirm the extra pass) Stress-test CuPy/GPU <code>python scripts/test_cupy.py --stress-seconds 10 --stress-size 4096</code> Scalar loop walkthrough <code>python scripts/quickstart_scalar.py --scenario basic --plot</code>"},{"location":"about/","title":"About","text":"<p>This project is a guided sandbox for my students to learn deep learning from the ground up. Instead of jumping straight into a large framework, you build every concept gradually:</p> <ul> <li>Scalar implementations show each neuron, gradient, and update explicitly.</li> <li>Vectorized code demonstrates how the same math translates into efficient NumPy/CuPy operations.</li> <li>Convolutional architectures introduce modern CNN design patterns without hiding the underlying modules.</li> </ul>"},{"location":"about/#learning-philosophy","title":"Learning Philosophy","text":"<ol> <li>Start simple: work through the scalar notebooks/scripts to understand backpropagation step by step.</li> <li>Scale up carefully: move to vectorized and convolutional modules once the math is clear.</li> <li>Experiment relentlessly: use the quick-start scripts, optimization lab, and augmentation playground to test hypotheses.</li> <li>Document everything: fill out the Experiment Log Template after each run to build scientific rigor.</li> </ol>"},{"location":"about/#how-to-use-this-page","title":"How to Use This Page","text":"<ul> <li>Share this page with new students as an overview of why the repository exists.</li> <li>Reference it when explaining course structure or when introducing new sections (Optimizers, Architecture Gallery, etc.).</li> <li>Pair it with the Debug Playbook so learners know where to go when they get stuck.</li> </ul>"},{"location":"activation-functions/","title":"Activation Functions","text":"<p>This page compares the activation functions implemented across the project so you can see how they shape neuron outputs and gradients. Each section includes the definition, derivative, usage context, and a quick snippet for plotting in a notebook.</p> <p>Interactive plots</p> <p>Copy the sample code cells into a Jupyter notebook (see the Optimization Lab) for interactive plots. The equations below match what you\u2019ll find in <code>convolutional/modules.py</code>.</p>"},{"location":"activation-functions/#1-relu-rectified-linear-unit","title":"1. ReLU (Rectified Linear Unit)","text":"<ul> <li>Definition: \\( \\mathrm{ReLU}(x) = \\max(0, x) \\)</li> <li>Derivative: \\( \\frac{d}{dx} = 1 \\) for \\( x &gt; 0 \\), \\( 0 \\) otherwise.</li> <li>Usage: Default activation throughout BaselineCNN, LeNet, AlexNet, VGG16, ResNet18, and many MLP layers.</li> <li>Intuition: Fast, sparse activations; prevents vanishing gradients for positive inputs.</li> </ul> <pre><code>import numpy as np, matplotlib.pyplot as plt\nx = np.linspace(-3, 3, 400)\ny = np.maximum(0, x)\ndy = (x &gt; 0).astype(float)\nplt.plot(x, y, label=\"ReLU\")\nplt.plot(x, dy, label=\"Derivative\", linestyle=\"--\")\nplt.legend(); plt.grid(True); plt.show()\n</code></pre>"},{"location":"activation-functions/#2-leaky-relu","title":"2. Leaky ReLU","text":"<ul> <li>Definition: \\( \\max(\\alpha x, x) \\) with small \\( \\alpha \\) (e.g., 0.01).</li> <li>Derivative: \\( 1 \\) for \\( x &gt; 0 \\), \\( \\alpha \\) otherwise.</li> <li>Usage: Available for experimentation in the MLP modules; useful when you worry about \u201cdead\u201d ReLUs.</li> <li>Intuition: Allows a trickle of gradient on negative inputs so neurons stay active.</li> </ul> <pre><code>alpha = 0.1\ny = np.where(x &gt; 0, x, alpha * x)\ndy = np.where(x &gt; 0, 1.0, alpha)\n</code></pre>"},{"location":"activation-functions/#3-silu-swish","title":"3. SiLU / Swish","text":"<ul> <li>Definition: \\( \\mathrm{SiLU}(x) = x \\cdot \\sigma(x) \\), where \\( \\sigma(x) \\) is the sigmoid.</li> <li>Derivative: \\( \\sigma(x) + x \\cdot \\sigma(x) \\cdot (1 - \\sigma(x)) \\)</li> <li>Usage: EfficientNet-Lite0 MBConv blocks (<code>convolutional/modules.py</code> supplies a <code>SiLU</code> class).</li> <li>Intuition: Smooth, non-monotonic activation; often improves accuracy in lightweight networks.</li> </ul> <pre><code>sig = 1 / (1 + np.exp(-x))\ny = x * sig\ndy = sig + x * sig * (1 - sig)\n</code></pre>"},{"location":"activation-functions/#4-gelu-gaussian-error-linear-unit","title":"4. GELU (Gaussian Error Linear Unit)","text":"<ul> <li>Definition: \\( \\mathrm{GELU}(x) = 0.5 x \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} (x + 0.044715 x^3)\\right)\\right) \\) (approximation used in practice).</li> <li>Derivative: More complex; frameworks typically compute it automatically (see <code>GELU</code> class for the exact expression).</li> <li>Usage: ConvNeXt blocks in <code>convolutional/architectures/convnext.py</code>.</li> <li>Intuition: Smooths the activation transition and behaves well with LayerNorm-heavy architectures.</li> </ul> <pre><code>import numpy as np\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\ny = gelu(x)\n</code></pre>"},{"location":"activation-functions/#quick-comparison-table","title":"Quick Comparison Table","text":"Activation Pros Cons Typical Use ReLU Simple, fast, prevents vanishing gradients \u201cDead\u201d neurons for large negative weights Default in most CNN/MLP layers Leaky ReLU Keeps a gradient on negative side Extra hyperparameter \\( \\alpha \\) Experimental variants of CNN/MLP SiLU / Swish Smooth, empirically strong in small nets Slightly more compute (sigmoid) EfficientNet-style blocks GELU Smooth, transformer-friendly More expensive to compute, derivative complicated ConvNeXt, transformer-inspired CNNs"},{"location":"activation-functions/#suggested-notebook-exercise","title":"Suggested Notebook Exercise","text":"<ol> <li>Copy the plotting snippets into a single Jupyter cell.</li> <li>Plot all four activations and their derivatives on the same axes.</li> <li>Feed a random batch through each activation and measure the mean/std of the outputs\u2014note how ReLU truncates negatives while SiLU/GELU keep them but dampen their magnitude.</li> </ol> <p>Document your observations in the notebook so you can refer back when choosing activations for custom architectures.</p>"},{"location":"architecture-gallery/","title":"Architecture Gallery","text":"<p>This page is the entry point to detailed write-ups for every convolutional network in the sandbox. Each model now has its own tutorial with block diagrams, teaching angles, experiment prompts, and references.</p>"},{"location":"architecture-gallery/#dataset-mnist-handwritten-digits","title":"Dataset: MNIST (Handwritten Digits)","text":"<ul> <li>Files: <code>data/train_images.npy</code>, <code>data/train_labels.npy</code>, <code>data/test_images.npy</code>, <code>data/test_labels.npy</code></li> <li>Shape: 28\u00d728 grayscale digits (0\u20139); 60k training + 10k test samples.</li> <li>Format: Stored as NumPy arrays for direct loading by <code>common/data_utils.DataUtility</code>, which also handles float32 conversion, normalization, and reshaping to <code>(N, 1, 28, 28)</code>.</li> <li>Reference: Read the data README for provenance and tips on creating validation splits.</li> </ul> <p>MNIST is intentionally simple so you can compare architectures rapidly without long training cycles.</p>"},{"location":"architecture-gallery/#architecture-index","title":"Architecture Index","text":"Model Era / Theme Tutorial BaselineCNN Custom lightweight starter Read notes LeNet-5 1998 classic digit recognizer Read notes AlexNet 2012 ImageNet breakthrough Read notes VGG-16 2014 deep uniform conv blocks Read notes ResNet-18 2015 residual learning Read notes EfficientNet-Lite0 2019 MBConv + SE efficiency Read notes ConvNeXt-Tiny 2022 transformer-inspired CNN Read notes"},{"location":"architecture-gallery/#comparing-architectures","title":"Comparing Architectures","text":"<p>Use this quick matrix to guide experiments:</p> Model Key innovations Suggested experiment LeNet Mean-pooling, shallow conv stacks Train 1\u20132 epochs; observe fast saturation on MNIST. AlexNet Deep conv stack, dropout, ReLU Time GPU vs CPU; discuss speed gains. VGG-16 Uniform 3\u00d73 conv blocks, depth Monitor memory usage vs accuracy payoff. ResNet-18 Skip connections for gradient flow Disable one skip (for learning) to see training degrade. EfficientNet-Lite0 Depthwise + SE, compound scaling Compare accuracy-per-parameter with VGG16. ConvNeXt-Tiny LayerNorm, GELU, large depthwise kernels Inspect misclassification patterns vs ResNet. <p>Document observations\u2014accuracy curves, loss plots, and timing tables make excellent lab artifacts.</p> <p>Pick any architecture above to dive into its dedicated tutorial and explore how the theoretical concepts translate into actual code.</p>"},{"location":"architecture-gallery/#lab-challenges","title":"Lab Challenges","text":"<ol> <li>Historical timeline: Choose three architectures from different eras (e.g., LeNet, VGG16, ConvNeXt) and create a comparison table that lists their parameter counts, training tricks, and MNIST accuracy after 2 epochs. Highlight one design idea that evolved between each pair.</li> <li>Residual vs non-residual: Train ResNet-18 for one epoch, then (for learning purposes only) comment out the skip connections in a single block and retrain. Document how loss and accuracy change, and explain why the skip path matters.</li> </ol>"},{"location":"augmentation-playground/","title":"Augmentation Playground","text":"<p>All three trainers now share the same augmentation stack via <code>common/augment.py</code>. The CLI front-ends expose identical knobs (see the new Augmentation Overview plus detailed guides on geometry, noise &amp; cutout, and CutMix/RandAugment).</p> <pre><code># scalar MLP\npython scalar/main.py --epochs 5 --augment-rotate-deg 12 --augment-cutout-prob 0.2\n\n# vectorized MLP\npython vectorized/main.py --epochs 2 --augment-cutmix-prob 0.5 --augment-cutmix-alpha 0.75\n\n# convolutional CNNs\npython convolutional/main.py resnet18 --augment-randaug-layers 2 --augment-randaug-magnitude 0.5\n</code></pre> <p>Every flag maps directly to the shared config (<code>max_shift</code>, <code>rotate_deg</code>, <code>noise_std</code>, <code>cutout_prob</code>, <code>cutmix_prob</code>, <code>randaugment_layers</code>, etc.). The scalar trainer disables CutMix internally to keep labels single-class, but it otherwise runs the same codepath as the higher-performance pipelines.</p> <p>Use <code>--no-augment</code> on any entry point to run with raw data (useful for ablations or unit tests).</p>"},{"location":"augmentation-playground/#1-understanding-the-existing-augmentor","title":"1. Understanding the Existing Augmentor","text":"<pre><code>def augment_image_batch(batch, cfg, xp_module=None, labels=None, allow_label_mix=True):\n    if batch.ndim not in (3, 4):\n        return batch, None\n    ...\n</code></pre> <ul> <li>Inputs can be <code>(N, C, H, W)</code> (conv/vectorized) or <code>(N, H, W)</code> (scalar) and the helper automatically reshapes as needed.</li> <li>Image ops (shift, rotate, flips, noise, cutout) run per-sample; CutMix optionally mixes labels if <code>allow_label_mix=True</code>.</li> <li><code>augment_flat_batch</code> reshapes <code>(N, 784)</code> vectors into <code>28\u00d728</code> grids for the vectorized trainer.</li> </ul> <p>Under the hood the trainers call:</p> <pre><code>xb, mix_meta = augment_image_batch(xb, cfg, xp_module=xp, labels=yb)\nlogits = model.forward(xb)\nloss, grad = combine_losses(logits, yb, mix_meta)\n</code></pre> <ul> <li>When <code>mix_meta</code> is <code>None</code>, the loss behaves like standard cross-entropy.</li> <li>When <code>mix_meta</code> contains CutMix metadata, both targets are blended during forward/backward passes.</li> <li>Scalar training requests <code>allow_label_mix=False</code>, so CutMix is skipped while other transforms still apply.</li> </ul>"},{"location":"augmentation-playground/#2-adding-rotations-small-angles","title":"2. Adding Rotations (Small Angles)","text":"<p>For MNIST, minor rotations (\u00b110\u00b0) keep label semantics intact. You can extend <code>_augment_batch</code>:</p> <pre><code>import math\n\ndef _augment_batch(self, xb, max_shift=2, max_rotate_deg=10):\n    if xb.ndim != 4:\n        return xb\n    shifted = xp.empty_like(xb)\n    for i in range(len(xb)):\n        dx = int(xp.random.randint(-max_shift, max_shift + 1))\n        dy = int(xp.random.randint(-max_shift, max_shift + 1))\n        img = xp.roll(xp.roll(xb[i], dy, axis=1), dx, axis=2)\n        if max_rotate_deg &gt; 0 and xp.random.rand() &lt; 0.5:\n            angle = float(xp.random.uniform(-max_rotate_deg, max_rotate_deg))\n            img = rotate_nearest(img, math.radians(angle))\n        shifted[i] = img\n    return shifted\n\ndef rotate_nearest(img, angle):\n    # xp-based nearest-neighbor rotation (adapted from scalar trainer for GPU support)\n    C, H, W = img.shape\n    cy, cx = (H - 1) / 2.0, (W - 1) / 2.0\n    yy, xx = xp.meshgrid(xp.arange(H), xp.arange(W), indexing=\"ij\")\n    x0, y0 = xx - cx, yy - cy\n    c, s = math.cos(angle), math.sin(angle)\n    xr = c * x0 + s * y0 + cx\n    yr = -s * x0 + c * y0 + cy\n    xi = xp.clip(xp.rint(xr).astype(int), 0, W - 1)\n    yi = xp.clip(xp.rint(yr).astype(int), 0, H - 1)\n    out = xp.empty_like(img)\n    for ch in range(C):\n        out[ch] = img[ch, yi, xi]\n    return out\n</code></pre> <p>Warning: Rotations beyond ~15\u00b0 may distort digits into different classes (e.g., \u201c6\u201d vs \u201c9\u201d). Always sanity-check a few augmented images using <code>plot_image_grid</code>.</p>"},{"location":"augmentation-playground/#3-horizontalvertical-flips-use-with-care","title":"3. Horizontal/Vertical Flips (Use with Care)","text":"<ul> <li>Digits: Horizontal flips can change semantics (\u201c2\u201d vs mirrored \u201c2\u201d). Avoid flips unless labels and dataset allow it.</li> <li>Natural images: For CIFAR-10 or larger datasets, flips often help. Implement by toggling axes:</li> </ul> <pre><code>if xp.random.rand() &lt; 0.5:\n    img = img[:, :, ::-1]  # horizontal flip\nif xp.random.rand() &lt; 0.1:\n    img = img[:, ::-1, :]  # vertical flip (rarely used for digits)\n</code></pre> <p>Always ask: Does a flipped image still belong to the same class? If not, skip the augmentation.</p>"},{"location":"augmentation-playground/#4-additive-noise","title":"4. Additive Noise","text":"<pre><code>noise_std = 0.05\nif noise_std &gt; 0:\n    img = img + noise_std * xp.random.randn(*img.shape).astype(img.dtype)\n    img = xp.clip(img, -3, 3)  # keep standardized range reasonable\n</code></pre> <ul> <li>Works well when data are standardized (mean\u22480, std\u22481). Adjust clipping bounds accordingly.</li> </ul>"},{"location":"augmentation-playground/#5-cutout-random-occlusion","title":"5. Cutout (Random Occlusion)","text":"<pre><code>def apply_cutout(img, size=5):\n    C, H, W = img.shape\n    x = xp.random.randint(0, W)\n    y = xp.random.randint(0, H)\n    x1 = max(0, x - size // 2)\n    x2 = min(W, x1 + size)\n    y1 = max(0, y - size // 2)\n    y2 = min(H, y1 + size)\n    img[:, y1:y2, x1:x2] = 0\n    return img\n</code></pre> <ul> <li>Caution: On small digits, large cutouts might erase the entire number. Start with size 3\u20135.</li> </ul>"},{"location":"augmentation-playground/#6-cutmix-and-randaugment","title":"6. CutMix and RandAugment","text":"<p>CutMix (vectorized + convolutional trainers):</p> <ul> <li>Controlled by <code>--augment-cutmix-prob</code> (probability per sample) and <code>--augment-cutmix-alpha</code> (Beta distribution for mixing ratio).</li> <li>Two images share a rectangular patch and the labels are blended using the CutMix ratio.</li> <li>The trainers compute <code>\u03bb * CE(y_a) + (1 - \u03bb) * CE(y_b)</code> and combine gradients accordingly.</li> <li>Disabled automatically for the scalar trainer (labels stay single-class there).</li> </ul> <p>RandAugment:</p> <ul> <li><code>--augment-randaug-layers</code> picks how many random ops to stack per sample.</li> <li><code>--augment-randaug-magnitude</code> acts as a 0\u20131 scaling factor for those ops.</li> <li>Available primitives: rotate, X/Y translations, flips, additive noise, cutout. Each layer randomly chooses one operation and scales its magnitude.</li> <li>Combine with the deterministic knobs (e.g., keep <code>--augment-rotate-deg</code> small so RandAugment explores meaningful ranges).</li> </ul> <p>Example:</p> <pre><code># more aggressive digits run\npython vectorized/main.py \\\n  --epochs 5 \\\n  --augment-randaug-layers 2 \\\n  --augment-randaug-magnitude 0.6 \\\n  --augment-cutout-prob 0.3 \\\n  --augment-cutmix-prob 0.2\n</code></pre> <p>Tune probabilities/thresholds per dataset and always visualize random batches to ensure semantics hold.</p>"},{"location":"augmentation-playground/#lab-challenge","title":"Lab Challenge","text":"<ol> <li>Use <code>--no-augment</code> as a baseline, then enable rotations + flips on each trainer. Record accuracy changes in the Experiment Log.</li> <li>Enable CutMix on the vectorized or convolutional trainer (<code>--augment-cutmix-prob 0.4</code>). Plot 10 mixed samples and explain why the label mixing math is required.</li> <li>Experiment with RandAugment (layers/magnitude sweep) and discuss whether it helps small digit datasets or if it simply injects too much randomness.</li> </ol> <p>Remember: augmentations are only beneficial when they preserve label semantics. Evaluate carefully before enabling them in production runs.</p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>This hub links to detailed tutorials covering every foundational concept used throughout the sandbox. Read them sequentially for a full theory-to-code walkthrough or jump directly to the topic you need.</p>"},{"location":"core-concepts/#concept-index","title":"Concept Index","text":"Topic What you\u2019ll learn Link Fundamentals Neurons, layers, and network composition in scalar vs vectorized code Study fundamentals Activation functions ReLU, LeakyReLU, SiLU, GELU, and softmax implementations Explore activations Loss &amp; softmax Numerically stable cross-entropy from logits and its gradients Understand loss Optimizers Hub for SGD, Adam, Lookahead, gradient clipping, and schedules Review optimizers Regularization Dropout, weight decay, data augmentation, and batch-size effects Apply regularization Normalization BatchNorm vs LayerNorm, when to use each, and why Compare normalization Convolution mechanics im2col/col2im, depthwise conv, squeeze-excite, global pooling Decode convolution Backend utilities NumPy \u2194 CuPy switching, device helpers, diagnostics Manage backends"},{"location":"core-concepts/#how-to-use-this-hub","title":"How to use this hub","text":"<ol> <li>Pick the concept you\u2019re studying in lecture.</li> <li>Read the linked tutorial to see the theory mapped onto exact files/functions.</li> <li>Modify the referenced code and run small experiments to solidify your understanding.</li> </ol> <p>With these guides, you can build a solid foundation before diving into the larger architectures and experiments.</p>"},{"location":"core-concepts/#lab-challenges","title":"Lab Challenges","text":"<ol> <li>Gradient trace: Follow the scalar implementation through a single training example (set batch size to 1). Write down the value of each neuron\u2019s activation and gradient, then verify they match the equations in the Fundamentals and Loss tutorials.</li> <li>Optimizer swap experiment: Using the Optimizer hub, implement both Adam and SGD+momentum for the same network, run each for 3 epochs, and plot/compare their loss histories. Summarize the behavioral differences you observed.</li> </ol>"},{"location":"debug-playbook/","title":"Debug Playbook","text":"<p>When experiments go sideways, start here. Each section lists common symptoms, likely causes, and quick fixes so you can get back to learning instead of fighting the environment.</p>"},{"location":"debug-playbook/#1-environment-installation","title":"1. Environment &amp; Installation","text":"Symptom Likely Cause Quick Fix <code>ModuleNotFoundError: No module named 'cupy'</code> CuPy not installed or wrong CUDA version Install the wheel matching your CUDA driver (<code>pip install cupy-cuda12x</code>). On CPU-only machines, omit <code>--gpu</code>. <code>ImportError: libcudart.so not found</code> CUDA toolkit not in PATH/LD_LIBRARY_PATH Install the CUDA runtime (matching the CuPy build) or add <code>/usr/local/cuda/lib64</code> to your <code>LD_LIBRARY_PATH</code>. <code>matplotlib</code> backend errors on headless servers Using default interactive backend without display Set <code>matplotlib.use(\"Agg\")</code> before importing pyplot or run the script with <code>--no-plot</code> / omit <code>--plot</code>."},{"location":"debug-playbook/#diagnostics","title":"Diagnostics","text":"<ul> <li><code>python scripts/test_system.py</code>: Basic sanity check of Python packages.</li> <li><code>python scripts/test_cupy.py --stress-seconds 5 --stress-size 2048</code>: Verifies CuPy can allocate memory, run kernels, and synchronize.</li> </ul>"},{"location":"debug-playbook/#2-gpu-specific-issues","title":"2. GPU-Specific Issues","text":""},{"location":"debug-playbook/#a-gpu-oom-out-of-memory","title":"A. GPU OOM (Out-Of-Memory)","text":""},{"location":"debug-playbook/#symptoms","title":"Symptoms","text":"<ul> <li>Training runs crash with <code>cupy.cuda.memory.OutOfMemoryError</code>.</li> <li>GPU utilization stays high after training (e.g., misclassification pass still computing).</li> </ul>"},{"location":"debug-playbook/#fixes","title":"Fixes","text":"<ol> <li>Reduce batch size (<code>--batch-size</code> flag) or switch to a smaller architecture.</li> <li>Disable optional post-processing (e.g., run <code>convolutional/main.py</code> without <code>--plot</code>/<code>--show-misclassified</code> to skip large visualization buffers).</li> <li>Clear lingering CuPy arrays by allowing scripts to exit, or manually call <code>backend.use_cpu()</code> when post-processing on CPU.</li> <li>Use the quick-start scripts with smaller scenarios to reproduce the issue quickly.</li> </ol>"},{"location":"debug-playbook/#b-gpu-utilization-stays-at-100-after-training","title":"B. GPU Utilization Stays at 100% After Training","text":"<p>Likely cause: The script is still running a heavy pass (e.g., collecting misclassifications). Remove plotting flags or run with <code>--show-misclassified</code> only when needed.</p>"},{"location":"debug-playbook/#c-cupy-fallback","title":"C. CuPy Fallback","text":"<p>If <code>--gpu</code> prints <code>[WARN] CuPy is not installed</code> and continues on CPU:</p> <ul> <li>Ensure <code>pip show cupy</code> returns a version matching your CUDA drivers.</li> <li>Test with <code>python scripts/test_cupy.py</code>; if it fails, reinstall CuPy or update drivers.</li> </ul>"},{"location":"debug-playbook/#3-data-shape-mismatches","title":"3. Data &amp; Shape Mismatches","text":"Symptom Likely Cause Quick Fix <code>ValueError: Axis dimension mismatch</code> in fully connected layers Flattened size doesn\u2019t match Dense layer input (e.g., VGG16 pooling stride mismatch) Recompute the flattened feature size; ensure the final Conv/Pool stages produce the expected spatial dimensions. The tutorial already patches VGG16 to track spatial size\u2014check similar math if you customize architectures. <code>RuntimeError: cannot reshape array of size ...</code> Loading dataset with different image shapes (e.g., CIFAR-10) but still reshaping to <code>(N, 1, 28, 28)</code> Update reshape logic to match the dataset: <code>(N, 3, 32, 32)</code> for RGB, <code>(N, height*width)</code> for flattened MLP inputs. Labels look incorrect (e.g., floats instead of ints) <code>.npy</code> files saved with wrong dtype Ensure labels are <code>int64</code> before saving. <code>DataUtility</code> casts to <code>int64</code>, but double-check custom datasets."},{"location":"debug-playbook/#tips","title":"Tips","text":"<ul> <li>Print tensor shapes after each major step (especially before Dense layers).</li> <li>Use <code>scripts/quickstart_* --scenario dataset-swap</code> to validate new datasets end-to-end.</li> </ul>"},{"location":"debug-playbook/#4-training-instability","title":"4. Training Instability","text":"Symptom Likely Cause Quick Fix Loss becomes <code>nan</code> or explodes Learning rate too high, no gradient clipping Lower <code>--lr</code>, enable <code>--grad-clip</code>, or switch to Adam for initial experiments. Accuracy stuck at chance Architecture too large, insufficient epochs, or data not normalized Verify <code>common/data_utils</code> normalization ran (call <code>DataUtility.load_data</code>), start with simpler nets, and ensure labels are correct. Training diverges when enabling GPU Uninitialized CuPy memory or mismatched dtype Confirm all inputs are <code>float32</code> (<code>DataUtility</code> already enforces this) and re-run the CuPy stress test."},{"location":"debug-playbook/#process-checklist","title":"Process Checklist","text":"<ol> <li>Run a scalar or vectorized quick-start scenario to ensure the math works.</li> <li>Move to the convolutional script once the dataset + optimizer combination is stable.</li> <li>Only then enable <code>--gpu</code>, <code>--lookahead</code>, or other advanced flags.</li> </ol>"},{"location":"debug-playbook/#5-checkpoint-serialization-problems","title":"5. Checkpoint &amp; Serialization Problems","text":"Symptom Likely Cause Quick Fix <code>KeyError</code> when loading a checkpoint Architecture changed between save/load Make sure you rebuild the exact same architecture (same <code>arch</code> flag, same hidden sizes) before calling <code>load_model</code>. Checkpoint files huge or missing metadata Saving every epoch or omitting metadata Use <code>--save</code> only when needed and pass <code>metadata</code> (already done in main scripts). Clean unused checkpoints from <code>checkpoints/</code>."},{"location":"debug-playbook/#resume-workflow","title":"Resume Workflow","text":"<ul> <li>Save after an initial run (<code>--save checkpoints/demo.npz</code>).</li> <li>Later, resume with <code>--load checkpoints/demo.npz --skip-train</code> to evaluate or <code>--epochs 1</code> to continue training.</li> </ul>"},{"location":"debug-playbook/#6-quick-reference-commands","title":"6. Quick Reference Commands","text":"Goal Command Validate CPU environment <code>python scripts/test_system.py</code> Stress-test GPU/CuPy <code>python scripts/test_cupy.py --stress-seconds 10 --stress-size 4096</code> Baseline scalar run <code>python scripts/quickstart_scalar.py --scenario basic --plot</code> Optimizer comparison (vectorized) <code>python scripts/quickstart_vectorized.py --scenario optimizer-compare --plot</code> GPU-ready CNN demo <code>python scripts/quickstart_convolutional.py --scenario gpu-fast --lookahead --plot</code> Fashion-MNIST swap smoke test <code>python scripts/quickstart_scalar.py --scenario dataset-swap --plot</code> <p>Keep this playbook handy whenever you run into trouble\u2014most issues trace back to one of the scenarios above. If you discover a new pitfall, add it here (noting the symptoms, cause, and fix) to help the next learner.</p>"},{"location":"experiment-log-template/","title":"Experiment Log Template","text":"<p>Use this template to record every meaningful run. Consistent notes help you reproduce results, compare hyperparameters, and communicate findings to classmates or your instructor.</p> <pre><code># Experiment Title\n\n- **Date:** YYYY-MM-DD\n- **Author:** Your name\n- **Goal:** Describe the purpose (e.g., \u201cCompare SGD vs Adam on vectorized MLP\u201d, \u201cTest gradient clipping on ResNet18\u201d).\n\n## Setup\n\n- **Dataset:** MNIST / Fashion-MNIST / CIFAR-10 (include file names if custom)\n- **Architecture:** e.g., scalar MLP (128,64), vectorized MLP (256,128), ResNet18, etc.\n- **Script/Command:** `python convolutional/main.py ...` or quick-start script invocation\n- **Hardware:** CPU model, GPU model (if any), environment (Conda env name, Python version)\n\n## Hyperparameters\n\n| Parameter | Value |\n| --- | --- |\n| Epochs |  |\n| Batch size |  |\n| Optimizer |  |\n| Learning rate |  |\n| Weight decay |  |\n| Gradient clipping |  |\n| Lookahead |  |\n| Augmentation |  |\n| Other |  |\n\n## Results\n\n- **Training loss (per epoch):** `[ ... ]`\n- **Validation/Test accuracy:** `... %`\n- **Runtime:** `... min`\n- **Notes on logs:** (e.g., loss spikes, warnings, CUDA messages)\n\n## Observations\n\n- What went well?\n- What didn\u2019t?\n- Any anomalies or bugs?\n- Next steps (follow-up experiments, parameters to try next)\n\n## Attachments (optional)\n\n- Paste charts (loss curves, misclassification grids) or link to screenshots.\n- If using Jupyter, note the notebook name and cell numbers.\n</code></pre> <p>Fill in one template per experiment and store them under <code>experiments/</code> or a personal <code>notes/</code> folder. Reviewing these logs before exams or project milestones will save you hours of guesswork.</p>"},{"location":"implementations-and-hardware/","title":"Implementations & Hardware","text":"<p>One of the sandbox\u2019s teaching pillars is showing how the same neural network evolves from na\u00efve loops to GPU-accelerated kernels. This module explains each stage and how to toggle between CPU and GPU execution.</p>"},{"location":"implementations-and-hardware/#scalar-loop-based-implementation","title":"Scalar (\u201cLoop-Based\u201d) Implementation","text":"<ul> <li>Location: <code>scalar/</code></li> <li>What it teaches: Each neuron, layer, and gradient update is implemented with explicit Python loops. Files like <code>scalar/neuron.py</code>, <code>scalar/layer.py</code>, and <code>scalar/trainer.py</code> print intermediate tensors so you can trace forward/backward propagation exactly as described in lecture.</li> <li>Takeaway: Perfect for debugging your intuition. You can literally follow how the derivative of the loss with respect to a weight is computed.</li> <li>Trade-off: Extremely slow\u2014only intended for tiny batches, but that is the point.</li> </ul>"},{"location":"implementations-and-hardware/#vectorized-implementation","title":"Vectorized Implementation","text":"<ul> <li>Location: <code>vectorized/</code></li> <li>What it teaches: Rewrites the scalar math using NumPy arrays (and optionally CuPy). Files such as <code>vectorized/modules.py</code> and <code>vectorized/optim.py</code> replace loops with matrix multiplications and broadcasted operations.</li> <li>Benefits: Orders-of-magnitude faster while remaining mathematically identical to the scalar version. Demonstrates why linear algebra primitives are the workhorse of deep learning.</li> </ul> <p>Runtime comparison</p> <p>Run <code>python vectorized/main.py --epochs 5 --batch-size 128</code> and compare runtime against the scalar script.</p>"},{"location":"implementations-and-hardware/#convolutional-stack","title":"Convolutional Stack","text":"<ul> <li>Location: <code>convolutional/</code></li> <li>What it teaches: Real-world CNNs built from custom Conv2D, pooling, normalization, activation, and dropout layers (<code>convolutional/modules.py</code>). Complex architectures such as LeNet, AlexNet, VGG16, ResNet18, EfficientNet-Lite0, and ConvNeXt-Tiny live in <code>convolutional/architectures/</code>.</li> <li>Trainer: <code>convolutional/trainer.py</code> unifies data shuffling, augmentation, LR schedules, backprop, and evaluation for every architecture. It respects the active backend (CPU vs GPU) automatically.</li> </ul>"},{"location":"implementations-and-hardware/#backend-switching-cpu-gpu","title":"Backend Switching (CPU \u2194 GPU)","text":""},{"location":"implementations-and-hardware/#the-xp-abstraction","title":"The <code>xp</code> Abstraction","text":"<ul> <li><code>common/backend.py</code> exposes an <code>xp</code> proxy that points to NumPy by default.</li> <li>Calling <code>backend.use_gpu()</code> swaps <code>xp</code> to CuPy (if available) and flips helper functions (<code>to_device</code>, <code>to_cpu</code>) accordingly.</li> <li>All downstream layers/optimizers import <code>xp</code>, so the implementation code does not need to care whether it\u2019s running on CPU or GPU.</li> </ul>"},{"location":"implementations-and-hardware/#when-to-use-gpu","title":"When to Use <code>--gpu</code>","text":"<ul> <li>Any <code>convolutional/main.py</code> run accepts <code>--gpu</code>. If CuPy is installed and a CUDA device is visible, training happens on the GPU; otherwise the script gracefully falls back to CPU.</li> <li>Vectorized scripts can also benefit by calling <code>backend.use_gpu()</code> before constructing modules (or by setting an environment variable if you prefer).</li> </ul>"},{"location":"implementations-and-hardware/#verifying-your-gpu-setup","title":"Verifying Your GPU Setup","text":"<p>Run the provided stress test to sanity-check drivers, CUDA, and CuPy:</p> <pre><code>python scripts/test_cupy.py --stress-seconds 10 --stress-size 4096\n</code></pre> <p>You can dial up <code>--stress-seconds</code> or <code>--stress-size</code> to keep the GPU busy longer. The script reports device info, runs elementwise ops, matmuls, custom kernels, and a configurable GEMM loop, exiting non-zero if anything fails.</p>"},{"location":"implementations-and-hardware/#summary-table","title":"Summary Table","text":"Implementation Location Highlights Scalar MLP <code>scalar/</code> Pedagogical loops, verbose printing, easiest to debug. Vectorized MLP <code>vectorized/</code> NumPy/CuPy arrays, fast batch math, same equations as scalar. CNN Suite <code>convolutional/</code> Custom layers, modern architectures, unified trainer. Backend Layer <code>common/backend.py</code> <code>xp</code> proxy, <code>use_gpu/use_cpu</code>, device transfers. GPU Health Check <code>scripts/test_cupy.py</code> Confirms CuPy can allocate, compute, and stress your GPU. <p>Understanding these tiers equips you to reason about both the what (network math) and the how (performance engineering).</p>"},{"location":"implementations-and-hardware/#lab-challenges","title":"Lab Challenges","text":"<ol> <li>Benchmark the tiers: Time one epoch of the scalar, vectorized, and convolutional implementations (use <code>scripts/quickstart_*</code> if you prefer). Record the wall-clock duration and explain the speed differences in your own words.</li> <li>Backend toggle drill: Run <code>python convolutional/main.py resnet18 --epochs 1 --batch-size 64</code> twice\u2014once with <code>--gpu</code>, once without. Note the runtime, GPU utilization (via <code>nvidia-smi</code>), and any startup warnings. Summarize the steps the backend layer took to select the device.</li> </ol>"},{"location":"optimization-lab/","title":"Optimization Lab","text":"<p>This lab is designed to be run inside a Jupyter notebook so you can tweak hyperparameters interactively. The notebook uses the vectorized MLP (NumPy-based) so the code stays concise while still reflecting all the optimizer features available in the main project.</p>"},{"location":"optimization-lab/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Jupyter (e.g., <code>pip install notebook</code>).</li> <li>In the repository root, launch <code>jupyter notebook</code>.</li> <li>Create a new notebook and copy increasingly complex cells from the sections below.</li> </ol> <p>Environment reminder</p> <p>Keep the repo\u2019s virtual environment activated before launching Jupyter so it can import project modules.</p>"},{"location":"optimization-lab/#1-setup-data-loading","title":"1. Setup &amp; Data Loading","text":"<pre><code>import os, sys, numpy as np\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))  # adjust if notebook lives elsewhere\n\nfrom common.data_utils import DataUtility\nfrom vectorized.modules import Sequential, Linear, ReLU\nfrom vectorized.optim import Adam, SGD, Lookahead\nfrom vectorized.trainer import VTrainer\n\ndata = DataUtility(\"data\")\nX_train, y_train, X_test, y_test = data.load_data()\nX_train = X_train.reshape(len(X_train), -1).astype(np.float32)\nX_test = X_test.reshape(len(X_test), -1).astype(np.float32)\n</code></pre> <ul> <li>Why: Loads MNIST, standardizes it, and flattens the images for the MLP.</li> </ul>"},{"location":"optimization-lab/#2-baseline-model","title":"2. Baseline Model","text":"<pre><code>def build_mlp(hidden_sizes=(256, 128)):\n    layers = []\n    in_dim = 28 * 28\n    for h in hidden_sizes:\n        layers.append(Linear(in_dim, h, activation_hint=\"relu\"))\n        layers.append(ReLU())\n        in_dim = h\n    layers.append(Linear(in_dim, 10, activation_hint=None))\n    return Sequential(*layers)\n\nbaseline_model = build_mlp()\nbaseline_optim = Adam(lr=1e-3, weight_decay=1e-4)\ntrainer = VTrainer(baseline_model, baseline_optim, num_classes=10, grad_clip_norm=None)\ntrainer.train(X_train, y_train, epochs=3, batch_size=64, verbose=True)\nacc = trainer.evaluate(X_test, y_test)\nprint(f\"Baseline accuracy: {acc*100:.2f}%\")\n</code></pre> <ul> <li>Experiment: Vary <code>hidden_sizes</code>, <code>epochs</code>, or <code>batch_size</code> to see how capacity and training time change.</li> </ul>"},{"location":"optimization-lab/#3-learning-rate-sweep","title":"3. Learning Rate Sweep","text":"<p>Use a small loop to try several learning rates quickly:</p> <pre><code>def train_with_lr(lr):\n    model = build_mlp()\n    optim = Adam(lr=lr, weight_decay=1e-4)\n    trainer = VTrainer(model, optim, num_classes=10)\n    trainer.train(X_train, y_train, epochs=2, batch_size=64, verbose=False)\n    return trainer.loss_history, trainer.evaluate(X_test, y_test)\n\nfor lr in [1e-4, 5e-4, 1e-3, 5e-3]:\n    loss_history, acc = train_with_lr(lr)\n    print(f\"lr={lr:.4g} -&gt; final loss={loss_history[-1]:.4f}, acc={acc*100:.2f}%\")\n</code></pre> <ul> <li>Observation: A learning rate that\u2019s too high may explode; too low converges slowly. Plot <code>loss_history</code> to visualize this.</li> </ul>"},{"location":"optimization-lab/#4-gradient-clipping-demo","title":"4. Gradient Clipping Demo","text":"<pre><code>model = build_mlp()\noptim = Adam(lr=1e-3)\ntrainer = VTrainer(model, optim, num_classes=10, grad_clip_norm=1.0)\ntrainer.train(X_train, y_train, epochs=3, batch_size=64, verbose=True)\n</code></pre> <ul> <li>Challenge: Try <code>grad_clip_norm=None</code> vs <code>grad_clip_norm=1.0</code> and note whether loss spikes disappear.</li> </ul>"},{"location":"optimization-lab/#5-lookahead-wrapper","title":"5. Lookahead Wrapper","text":"<pre><code>base_opt = Adam(lr=1e-3, weight_decay=1e-4)\nlookahead_opt = Lookahead(base_opt, k=5, alpha=0.5)\nmodel = build_mlp()\ntrainer = VTrainer(model, lookahead_opt, num_classes=10, grad_clip_norm=5.0)\ntrainer.train(X_train, y_train, epochs=3, batch_size=64, verbose=True)\ntrainer.evaluate(X_test, y_test)\n</code></pre> <ul> <li>Activity: Adjust <code>k</code> and <code>alpha</code>, then compare loss curves to plain Adam. Does Lookahead smooth convergence?</li> </ul>"},{"location":"optimization-lab/#6-visualization-optional","title":"6. Visualization (Optional)","text":"<pre><code>import matplotlib.pyplot as plt\n\nplt.plot(range(1, len(trainer.loss_history)+1), trainer.loss_history, marker=\"o\")\nplt.title(\"Loss curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.show()\n\nimgs, preds, trues, total = trainer.misclassification_data(X_test, y_test, max_images=25)\n</code></pre> <ul> <li>Warning: Collecting misclassifications requires another forward pass; only do this when necessary.</li> </ul>"},{"location":"optimization-lab/#suggested-notebook-challenges","title":"Suggested Notebook Challenges","text":"<ol> <li>Optimizer grid: For learning rates <code>[1e-4, 1e-3, 5e-3]</code> and optimizers <code>{SGD+momentum, Adam, Lookahead(Adam)}</code>, fill in an accuracy table and highlight the best combo.</li> <li>Clip vs No Clip: Run the same model with <code>grad_clip_norm=None</code> and <code>grad_clip_norm=1</code>. Plot both loss curves on the same axes and describe the difference.</li> <li>Lookahead intuition: Fix the base optimizer and vary <code>k</code> (e.g., 3, 5, 10) to see how frequently syncing slow weights affects convergence speed.</li> <li>Batch size stress-test: Keep epochs constant but sweep batch sizes <code>[32, 64, 128, 256]</code>. Track runtime and accuracy to discuss the trade-offs.</li> </ol> <p>Save the notebook as <code>notebooks/optimization_lab.ipynb</code> (or similar) so future students can open it, rerun the cells, and add their own observations.</p>"},{"location":"performance-profiles/","title":"Performance Profiles","text":"<p>Use this page as a reference for typical training times and accuracies across architectures. The numbers below were collected on a representative development machine (Intel i7-11700K CPU, NVIDIA RTX 3060 GPU) using the default settings from the main scripts. Your hardware may differ, but these baselines help you spot configurations that are way off.</p> <p>Timing scope</p> <p>Times include only the training loop (not plotting or misclassification collection). GPU runs use <code>--gpu</code> with CuPy installed.</p>"},{"location":"performance-profiles/#scalar-vectorized-mlps","title":"Scalar &amp; Vectorized MLPs","text":"Model Epochs Batch Size CPU Time (approx) GPU Time Test Accuracy Scalar MLP <code>(128,64)</code> 1 64 ~5 min N/A 85\u201390% Scalar MLP <code>(128,64)</code> 5 64 ~25 min N/A 93\u201395% Vectorized MLP <code>(256,128)</code> 1 128 ~40 sec ~8 sec 95\u201396% Vectorized MLP <code>(256,128)</code> 5 128 ~3 min ~40 sec 97\u201398% <p>Checklist: If your vectorized run takes 10\u00d7 longer than the table, double-check that NumPy is built for your CPU, or switch to <code>--gpu</code>.</p>"},{"location":"performance-profiles/#convolutional-architectures-cpu-vs-gpu","title":"Convolutional Architectures (CPU vs GPU)","text":"Architecture Epochs Batch Size CPU Time GPU Time Test Accuracy BaselineCNN 1 64 ~2 min ~30 sec 98% LeNet-5 1 64 ~3 min ~45 sec 98% AlexNet 1 64 ~10 min ~1.5 min 99% VGG16 1 64 ~25 min ~3.5 min 99% ResNet18 1 64 ~18 min ~2.5 min 99% EfficientNet-Lite0 1 64 ~20 min ~3 min 99% ConvNeXt-Tiny 1 64 ~30 min ~4 min 99% <p>Notes:</p> <ul> <li>Times scale roughly linearly with epochs. Multiply by 5 for a 5-epoch estimate.</li> <li>GPU speedup varies with architecture depth; bigger models benefit more.</li> <li>For ResNet/ConvNeXt, consider enabling gradient clipping (<code>--grad-clip 5</code>) to avoid rare divergence.</li> </ul>"},{"location":"performance-profiles/#quick-guidance","title":"Quick Guidance","text":"<ul> <li>CPU vs GPU delta: Expect 4\u20138\u00d7 speedups for CNNs. If the GPU run is slower than CPU, check that CuPy is actually active (look for \u201cGPU backend enabled via CuPy\u201d message).</li> <li>Accuracy sanity check: All networks should exceed 98% on MNIST after 1\u20132 epochs. Significantly lower accuracy usually indicates missing normalization, mislabeled data, or an incorrect architecture configuration.</li> <li>Batch size impact: Doubling the batch size roughly halves the number of steps per epoch but may hurt generalization slightly. Monitor accuracy when changing it.</li> </ul>"},{"location":"performance-profiles/#lab-exercise","title":"Lab Exercise","text":"<ol> <li>Pick two architectures (one shallow, one deep) and reproduce the 1-epoch CPU and GPU timings on your own hardware. Record the numbers in your Experiment Log.</li> <li>Explain any deviations from the table (e.g., slower GPU due to integrated graphics, faster CPU due to more cores).</li> <li>Share your findings with classmates to build a classroom-wide performance reference.</li> </ol>"},{"location":"project-tour/","title":"Project Tour","text":"<p>This project is designed as a teaching sandbox. Each directory demonstrates a different perspective on neural networks\u00c3\u00a2\u00e2\u201a\u00ac\u00e2\u20ac\u009dfrom slow-but-transparent scalar math to production-style CNN stacks. Use this tour to map the theoretical lectures to the exact files you should inspect.</p>"},{"location":"project-tour/#top-level-layout","title":"Top-Level Layout","text":"Path Why it matters <code>scalar/</code> Loop-based multilayer perceptron (MLP). Every neuron, gradient, and update is spelled out with Python <code>for</code> loops so you can follow the math line by line. The default stack now uses a 256-128-64 topology with activation-aware (He/Xavier) initialization and optional dropout after each hidden layer. <code>vectorized/</code> NumPy/CuPy version of the same MLP. Shows how replacing loops with matrix operations unlocks massive speed-ups while preserving feature parity: per-layer activation selection, dropout scheduling, and optional batch normalization. <code>convolutional/</code> CNN modules (<code>convolutional/modules.py</code>), a unified trainer, and the architecture definitions living under <code>convolutional/architectures/</code>. <code>common/</code> Shared backend selector, losses, softmax, model I/O, and data utilities. Every other package imports from here to stay consistent. <code>data/</code> Stores the MNIST <code>.npy</code> files (<code>train_images.npy</code>, <code>train_labels.npy</code>, etc.) plus a data README describing the dataset. <code>scripts/</code> One-off tools such as GPU diagnostics (<code>test_cupy.py</code>), experiment helpers, and sanity checks. <code>checkpoints/</code> Optional folder (create it as needed) where training runs can save <code>.npz</code> weight files."},{"location":"project-tour/#fully-connected-feature-flags","title":"Fully Connected Feature Flags","text":"<p>Both the scalar and vectorized entry points expose identical controls so you can prototype architectures from the CLI:</p> <ul> <li><code>--hidden-sizes 256,128,64</code> changes the per-layer width (defaults already match this expanded topology).</li> <li><code>--hidden-activations relu,tanh,gelu</code> mixes activations per layer; each layer automatically receives the matching He/Xavier initialization in <code>scalar/neuron.py</code> and <code>vectorized/modules.Linear</code>.</li> <li><code>--hidden-dropout 0.3,0.2,0.1</code> (scalar) or <code>--dropout 0.3,0.2,0.1</code> (vectorized) sets the keep probability after each hidden layer.</li> <li><code>--batchnorm</code> (vectorized) inserts <code>BatchNorm1D</code> modules between the Linear and activation stages; tweak <code>--bn-momentum</code> to adjust the running-stat update rate.</li> </ul> <p>The quick-start scripts forward the same flags, so you can stage tutorial without editing source files.</p>"},{"location":"project-tour/#shared-utilities-common","title":"Shared Utilities (<code>common/</code>)","text":"<ul> <li>Backend selector (<code>common/backend.py</code>): provides the <code>xp</code> alias that points to either NumPy (CPU) or CuPy (GPU) plus helper functions (<code>to_device</code>, <code>to_cpu</code>, <code>gpu_available</code>, etc.).</li> <li>Data loader (<code>common/data_utils.py</code>): loads the MNIST <code>.npy</code> arrays, normalizes them (mean/std), reshapes images, and exposes quick sample-plotting helpers.</li> <li>Loss &amp; softmax (<code>common/cross_entropy.py</code>, <code>common/softmax.py</code>): numerically stable log-sum-exp implementation of cross-entropy and the associated gradients.</li> <li>Model I/O (<code>common/model_io.py</code>): serialize weights/metadata to <code>.npz</code> files and reload them later\u00c3\u00a2\u00e2\u201a\u00ac\u00e2\u20ac\u009dused by all trainers.</li> </ul>"},{"location":"project-tour/#dataset-storage-data","title":"Dataset Storage (<code>data/</code>)","text":"<p>MNIST is packaged as NumPy arrays to keep the focus on neural-network mechanics. Files include:</p> <ul> <li><code>train_images.npy</code> / <code>train_labels.npy</code>: 60k training samples (28\u00c3\u0192\u00e2\u20ac\u201d28 grayscale digits).</li> <li><code>test_images.npy</code> / <code>test_labels.npy</code>: 10k evaluation samples.</li> <li><code>index.md</code>: explains provenance, shapes, and how to create validation splits if desired.</li> </ul>"},{"location":"project-tour/#swapping-in-other-datasets","title":"Swapping in Other Datasets","text":"<p>You can drop additional datasets into the same folder as long as you convert them to <code>.npy</code> files. Two quick options:</p> <ul> <li>Fashion-MNIST (drop-in replacement) </li> <li>Download via <code>torchvision.datasets.FashionMNIST</code> or the official website.  </li> <li> <p>Save the tensors to <code>.npy</code>:</p> <pre><code>np.save(\"data/fashion_train_images.npy\", train_images.numpy())\nnp.save(\"data/fashion_train_labels.npy\", train_labels.numpy())\n</code></pre> </li> <li> <p>Run <code>DataUtility(\"data\").load_data(train_images_file=\"fashion_train_images.npy\", ...)</code>.   The scalar/vectorized/convolutional scripts will work without further changes because Fashion-MNIST shares MNIST\u00c3\u00a2\u00e2\u201a\u00ac\u00e2\u201e\u00a2s 28\u00c3\u0192\u00e2\u20ac\u201d28\u00c3\u0192\u00e2\u20ac\u201d1 shape.</p> </li> <li> <p>CIFAR-10 (RGB, 32\u00c3\u0192\u00e2\u20ac\u201d32) </p> </li> <li>Convert the training/test splits to <code>.npy</code> (e.g., via <code>torchvision.datasets.CIFAR10</code>).  </li> <li>Update the reshape lines in <code>vectorized/main.py</code> and <code>convolutional/main.py</code> to use <code>(len(X), 3, 32, 32)</code> for CNNs or <code>len(X), -1</code> for MLPs.  </li> <li>Adjust the first convolutional layer to accept 3 input channels (e.g., modify the architecture builder or create a new entry in <code>ARCH_REGISTRY</code>).  </li> <li>Normalize using dataset-specific mean/std to keep training stable.</li> </ul> <p>For any other dataset, follow the same pattern: convert to <code>.npy</code>, point <code>DataUtility</code> at the new filenames, and ensure model input shapes/first-layer channels match the data.</p>"},{"location":"project-tour/#fashion-mnist-walk-through","title":"Fashion-MNIST Walk-Through","text":"<p>Convert and run:</p> <pre><code># convert_fashion_mnist.py\nimport numpy as np\nfrom torchvision.datasets import FashionMNIST\n\ntrain = FashionMNIST(root=\"data/raw\", train=True, download=True)\ntest = FashionMNIST(root=\"data/raw\", train=False, download=True)\n\nnp.save(\"data/fashion_train_images.npy\", train.data.numpy())\nnp.save(\"data/fashion_train_labels.npy\", train.targets.numpy())\nnp.save(\"data/fashion_test_images.npy\", test.data.numpy())\nnp.save(\"data/fashion_test_labels.npy\", test.targets.numpy())\n</code></pre> <pre><code>python scripts/quickstart_scalar.py --scenario dataset-swap --plot \\\n    --alt-train-images fashion_train_images.npy \\\n    --alt-train-labels fashion_train_labels.npy \\\n    --alt-test-images fashion_test_images.npy \\\n    --alt-test-labels fashion_test_labels.npy\n</code></pre> <p>No additional changes needed because the images are still 28\u00c3\u0192\u00e2\u20ac\u201d28\u00c3\u0192\u00e2\u20ac\u201d1.</p>"},{"location":"project-tour/#cifar-10-walk-through","title":"CIFAR-10 Walk-Through","text":"<p>Convert RGB 32\u00c3\u0192\u00e2\u20ac\u201d32 data and reshape for CNNs:</p> <pre><code># convert_cifar10.py\nimport numpy as np\nfrom torchvision.datasets import CIFAR10\n\ntrain = CIFAR10(root=\"data/raw\", train=True, download=True)\ntest = CIFAR10(root=\"data/raw\", train=False, download=True)\n\nnp.save(\"data/cifar10_train_images.npy\", train.data.transpose(0, 3, 1, 2))\nnp.save(\"data/cifar10_train_labels.npy\", np.array(train.targets))\nnp.save(\"data/cifar10_test_images.npy\", test.data.transpose(0, 3, 1, 2))\nnp.save(\"data/cifar10_test_labels.npy\", np.array(test.targets))\n</code></pre> <pre><code>python scripts/quickstart_convolutional.py --scenario dataset-swap \\\n    --arch resnet18 --epochs 1 --batch-size 64 --plot \\\n    --alt-train-images cifar10_train_images.npy \\\n    --alt-train-labels cifar10_train_labels.npy \\\n    --alt-test-images cifar10_test_images.npy \\\n    --alt-test-labels cifar10_test_labels.npy \\\n    --image-shape 3,32,32\n</code></pre> <p>ResNet18 already handles 3-channel inputs when the data reshape matches <code>(3,32,32)</code>; confirm your custom architectures do the same.</p>"},{"location":"project-tour/#helper-scripts-scripts","title":"Helper Scripts (<code>scripts/</code>)","text":"<ul> <li><code>test_cupy.py</code>: stress-tests your CUDA+CuPy setup with basic ops, matmuls, kernel launches, and configurable GEMM stress loops (<code>--stress-seconds</code>, <code>--stress-size</code>).</li> <li><code>test_system.py</code>, <code>sanity_linear_toy.py</code>, <code>experiments.py</code>: minimal scripts for system diagnostics or quick experiments.</li> </ul>"},{"location":"project-tour/#lab-challenges","title":"Lab Challenges","text":"<ol> <li>Map the repo: Create a diagram (digital or hand-drawn) showing each top-level folder, two representative files inside it, and one sentence about what they do. Share it with a peer or TA to verify you captured every component.</li> <li>Dataset swap dry run: Convert Fashion-MNIST to <code>.npy</code>, place the files in <code>data/</code>, and run <code>scripts/quickstart_scalar.py --scenario dataset-swap --plot</code>. Record the resulting accuracy and any code/config changes you had to make.</li> </ol>"},{"location":"project-tour/#required-packages","title":"Required Packages","text":"<p>The sandbox intentionally keeps dependencies light. Install the following inside your Conda/virtualenv:</p> <ul> <li>Python &gt;= 3.10</li> <li>NumPy</li> <li>CuPy (optional, for GPU acceleration)</li> <li>Matplotlib (for training curves + misclassification grids)</li> <li>tqdm (progress bars)</li> </ul> <p>Standard-library modules (<code>argparse</code>, <code>pathlib</code>, <code>math</code>, <code>time</code>, etc.) round out the tooling.</p>"},{"location":"project-tour/#why-this-structure-matters","title":"Why This Structure Matters","text":"<p>The directory split mirrors the learning journey:</p> <ol> <li>Understand the math in <code>scalar/</code>.</li> <li>Learn vectorization in <code>vectorized/</code>.</li> <li>Scale up to CNNs in <code>convolutional/</code>.</li> <li>Rely on shared utilities in <code>common/</code> so all implementations behave the same.</li> </ol> <p>Refer back to this map whenever you encounter a concept in the tutorial; the table above tells you which file to inspect next.</p>"},{"location":"roadmap/","title":"Learning Roadmap","text":"<p>Use this step-by-step progression to work through the NNFS sandbox. Each stage links to the relevant tutorial page(s) and suggests commands to run before moving on.</p>"},{"location":"roadmap/#stage-0-environment-data","title":"Stage 0 \u2013 Environment &amp; Data","text":"<ol> <li>Install dependencies</li> </ol> <pre><code>pip install numpy matplotlib tqdm cupy  # CuPy optional if you have CUDA\n</code></pre> <ol> <li>Verify GPU health</li> </ol> <pre><code>python scripts/test_cupy.py --stress-seconds 5 --stress-size 2048\n</code></pre> <ol> <li>Check dataset</li> <li>Ensure <code>data/train_images.npy</code>, <code>data/train_labels.npy</code>, <code>data/test_images.npy</code>, <code>data/test_labels.npy</code> exist (see the data README for details).</li> </ol>"},{"location":"roadmap/#stage-1-project-orientation","title":"Stage 1 \u2013 Project Orientation","text":"<ol> <li>Read Project Tour to understand the directory layout and shared utilities.</li> <li>Skim Implementations &amp; Hardware for the scalar \u2192 vectorized \u2192 GPU progression.</li> <li>Review Core Concepts as needed (activations, losses, optimizers, regularization).</li> </ol>"},{"location":"roadmap/#stage-2-scalar-baseline","title":"Stage 2 \u2013 Scalar Baseline","text":"<ol> <li>Read Running Experiments, scalar section.</li> <li>Run the scalar trainer with defaults:</li> </ol> <pre><code> python scalar/main.py --epochs 1 --batch-size 64 --plot\n</code></pre> <ol> <li>Experiment with CLI flags: <code>--hidden-activations</code>, <code>--hidden-dropout</code>, <code>--no-augment</code>.</li> <li>Fill an entry in Experiment Log Template capturing loss, accuracy, runtime.</li> </ol>"},{"location":"roadmap/#stage-3-vectorized-mlp","title":"Stage 3 \u2013 Vectorized MLP","text":"<ol> <li>Read the vectorized section in Running Experiments.</li> <li>Train with validation split + confusion matrix:</li> </ol> <pre><code>python vectorized/main.py --epochs 3 --batch-size 128 \\\n  --val-split 0.2 --confusion-matrix \\\n  --augment-cutmix-prob 0.3 --augment-randaug-layers 1\n</code></pre> <ol> <li>Compare results to scalar runs; note the effect of batchnorm/dropout toggles.</li> <li>Review the Augmentation Playground and mini-guides under <code>tutorial/augmentations/</code> for policy ideas.</li> </ol>"},{"location":"roadmap/#stage-4-optimization-lab","title":"Stage 4 \u2013 Optimization Lab","text":"<ol> <li>Work through Optimization Lab in Jupyter (learning-rate sweeps, gradient clipping, Lookahead).</li> <li>Use the provided code snippets to instrument loss curves, then document findings in your experiment log.</li> </ol>"},{"location":"roadmap/#stage-5-convolutional-architectures","title":"Stage 5 \u2013 Convolutional Architectures","text":"<ol> <li>Read Architecture Gallery (at least BaselineCNN, LeNet, ResNet18).</li> <li>Train baseline/resnet with validation + confusion matrix:</li> </ol> <pre><code>python convolutional/main.py baseline --epochs 3 --batch-size 64 --gpu \\\n  --val-split 0.1 --confusion-matrix\n\npython convolutional/main.py resnet18 --epochs 3 --batch-size 64 --gpu \\\n  --augment-cutmix-prob 0.3 --augment-randaug-layers 2 --confusion-matrix\n</code></pre> <ol> <li>Use <code>--save</code> / <code>--load</code> to practice checkpointing, then inspect misclassifications with <code>--show-misclassified</code>.</li> </ol>"},{"location":"roadmap/#stage-6-tools-utilities","title":"Stage 6 \u2013 Tools &amp; Utilities","text":"<ol> <li>Dive into Tools Quick Notes (NumPy, matplotlib, tqdm, CuPy) to ensure you understand the ecosystem.</li> <li>Explore <code>scripts/</code>:</li> <li><code>scripts/quickstart_scalar.py --scenario basic</code></li> <li><code>scripts/quickstart_vectorized.py --scenario optimizer-compare</code></li> <li><code>scripts/quickstart_convolutional.py --scenario gpu-fast --lookahead</code></li> <li>Consider building a new script or preset augmentation config to automate future labs.</li> </ol>"},{"location":"roadmap/#stage-7-reflection-next-steps","title":"Stage 7 \u2013 Reflection &amp; Next Steps","text":"<ol> <li>Summarize key learnings in your experiment log (top 3 takeaways, biggest pain points).</li> <li>Plan follow-up ideas:</li> <li>Swap datasets (e.g., Fashion-MNIST using <code>DataUtility.load_data(...)</code> overrides).</li> <li>Add new architectures or augmentations.</li> <li>Automate metric logging (CSV/TensorBoard) or benchmarking.</li> </ol>"},{"location":"running-experiments/","title":"Running Experiments","text":"<p>This guide walks you through training the provided models, timing CPU vs GPU runs, and saving/loading checkpoints. Mix and match the commands to design your own lab exercises.</p>"},{"location":"running-experiments/#prerequisites","title":"Prerequisites","text":"<ol> <li>Activate the Conda/virtualenv where NumPy, CuPy (optional), matplotlib, and tqdm are installed.</li> <li>Place the MNIST <code>.npy</code> files inside <code>data/</code> (already provided in this repo).</li> <li>From the project root, run the commands below.</li> </ol> <p>Label sanity checks</p> <p>Label sanity checks now run automatically. If your labels are one-hot matrices or include values outside <code>[0, 9]</code>, the trainer raises a descriptive error before training starts.</p>"},{"location":"running-experiments/#reproducibility-monitoring","title":"Reproducibility &amp; Monitoring","text":"<ul> <li>All entry points accept <code>--seed</code> (default <code>42</code>). Set it explicitly (<code>--seed 1337</code>) to make shuffling, augmentation, and optimizer initialization deterministic across runs.</li> <li><code>--val-split</code> (default <code>0.1</code>) automatically carves out a validation subset from the training data; epoch logs now include both train and validation loss/accuracy.</li> <li>Append <code>--confusion-matrix</code> to dump class-by-class counts after the final test evaluation\u2014handy for spotting systematic mistakes.</li> </ul> <p>Example one-liners:</p> <pre><code># Scalar MLP with deterministic split + confusion matrix\npython scalar/main.py --epochs 2 --batch-size 64 \\\n    --seed 7 --val-split 0.15 --confusion-matrix\n\n# Vectorized MLP on GPU with CutMix + RandAugment, logging val metrics\npython vectorized/main.py --epochs 3 --batch-size 128 --gpu \\\n    --seed 2024 --val-split 0.2 --augment-cutmix-prob 0.3 \\\n    --augment-randaug-layers 2 --confusion-matrix\n\n# CNN baseline with validation monitoring and confusion matrix\npython convolutional/main.py baseline --epochs 3 --batch-size 64 --gpu \\\n    --seed 99 --val-split 0.1 --confusion-matrix\n</code></pre>"},{"location":"running-experiments/#fully-connected-networks","title":"Fully Connected Networks","text":"Scalar debug modeVectorized (NumPy/CuPy) mode <pre><code>python scalar/main.py --epochs 1 --batch-size 64 \\\n    --hidden-sizes 256,128,64 \\\n    --hidden-activations relu,leaky_relu,tanh \\\n    --hidden-dropout 0.3,0.2,0.1 \\\n    --plot\n</code></pre> <ul> <li>Still the pure-Python teaching model, now with activation-aware initialization (He for ReLU/LeakyReLU/GELU, Xavier for tanh/sigmoid) tied to each layer's selected activation.</li> <li><code>--hidden-activations</code> lets you mix activations per layer without touching source files; provide a comma-separated list that matches <code>--hidden-sizes</code>.</li> <li><code>--hidden-dropout</code> inserts dropout after every hidden block so you can demo regularization directly in the scalar path.</li> <li><code>--plot</code> remains opt-in; leave it off whenever you want to skip matplotlib windows.</li> </ul> <pre><code>python vectorized/main.py --epochs 3 --batch-size 128 --gpu \\\n    --hidden-sizes 512,256,128 \\\n    --hidden-activations relu,gelu,tanh \\\n    --dropout 0.2 \\\n    --batchnorm\n</code></pre> <ul> <li>Shares all scalar flags plus <code>--batchnorm</code> (adds <code>BatchNorm1D</code> between Linear and activation) and <code>--bn-momentum</code> to tune running-stat updates.</li> <li><code>--dropout</code> accepts either a single value or comma-separated list; semantics match <code>--hidden-dropout</code>.</li> <li><code>--leaky-negative-slope</code> controls the slope when you include LeakyReLU in the activation list.</li> <li><code>--lr-schedule cosine</code> / <code>--lr-schedule reduce_on_plateau</code> plus <code>--min-lr</code>, <code>--reduce-factor</code>, <code>--reduce-patience</code>, and <code>--reduce-delta</code> let you test advanced learning-rate policies; pair them with <code>--early-stopping --early-patience 4 --early-delta 5e-4</code> to halt when validation loss stalls (validation split driven by <code>--val-split</code>, default 0.1).</li> <li>Augmentation knobs are now built-in: tweak <code>--augment-max-shift</code>, <code>--augment-rotate-deg</code>, <code>--augment-hflip-prob</code>, <code>--augment-vflip-prob</code>, <code>--augment-noise-std</code>, etc., or disable everything with <code>--no-augment</code>.</li> </ul> <p>Vectorized trainer scope</p> <p>The vectorized trainer is CPU/NumPy-only today. Use the convolutional entrypoint (<code>convolutional/main.py --gpu ...</code>) when you need CuPy acceleration.</p>"},{"location":"running-experiments/#convolutional-architectures","title":"Convolutional Architectures","text":"<p>All CNNs share the entry point <code>convolutional/main.py</code>. The <code>arch</code> positional argument selects the model:</p> <pre><code>    baseline, lenet, alexnet, vgg16, resnet18, efficientnet_lite0, convnext_tiny\n</code></pre>"},{"location":"running-experiments/#quick-comparison-2-epochs-each","title":"Quick Comparison (2 epochs each)","text":"<pre><code>python convolutional/main.py baseline            --epochs 2 --batch-size 64 --gpu\npython convolutional/main.py lenet               --epochs 2 --batch-size 64 --gpu\npython convolutional/main.py alexnet             --epochs 2 --batch-size 64 --gpu\npython convolutional/main.py vgg16               --epochs 2 --batch-size 64 --gpu\npython convolutional/main.py resnet18            --epochs 2 --batch-size 64 --gpu\npython convolutional/main.py efficientnet_lite0  --epochs 2 --batch-size 64 --gpu\npython convolutional/main.py convnext_tiny       --epochs 2 --batch-size 64 --gpu\n</code></pre> <p>Observations to make:</p> <ul> <li>Training time vs architecture depth.</li> <li>Test accuracy after two epochs.</li> <li>GPU utilization compared to CPU runs (rerun without <code>--gpu</code> to see the difference).</li> <li>Want heavier augmentation? Add flags such as <code>--augment-rotate-deg 15 --augment-hflip-prob 0.5 --augment-noise-std 0.03</code> (or turn everything off with <code>--no-augment</code>).</li> </ul>"},{"location":"running-experiments/#save-resume-workflow","title":"Save / Resume Workflow","text":"<pre><code>mkdir -p checkpoints\npython convolutional/main.py resnet18 --epochs 2 --batch-size 64 --gpu \\\n    --save checkpoints/resnet18_e2.npz\n\npython convolutional/main.py resnet18 --epochs 1 --batch-size 64 --gpu \\\n    --load checkpoints/resnet18_e2.npz \\\n    --save checkpoints/resnet18_e3.npz\n</code></pre> <ul> <li>The first command trains for two epochs and saves the weights.</li> <li>The second loads the checkpoint, trains one extra epoch, and writes a new file.</li> <li>Metadata (arch name, epoch count) is embedded via <code>common/model_io.py</code>.</li> </ul>"},{"location":"running-experiments/#quick-start-scripts","title":"Quick-Start Scripts","text":"<p>Prefer a guided walkthrough? Each module has a scenario-driven helper in <code>scripts/</code>:</p> Scalar quickstartVectorized quickstartConvolutional quickstart <pre><code>python scripts/quickstart_scalar.py --scenario basic --epochs 1 \\\n    --hidden-activations relu,gelu,tanh --hidden-dropout 0.25 --plot\n</code></pre> <p>Loads MNIST, previews a handful of samples, trains the scalar MLP, and optionally plots loss/prediction grids. Alternate scenarios compare optimizers or plug in Fashion-MNIST style <code>.npy</code> files, and every scenario now honors the <code>--hidden-sizes</code>, <code>--hidden-activations</code>, and <code>--hidden-dropout</code> flags.</p> <pre><code>python scripts/quickstart_vectorized.py --scenario hidden-sweep --batchnorm \\\n    --dropout 0.2 --lr-schedule cosine --val-split 0.1 --early-stopping \\\n    --augment-rotate-deg 15 --augment-noise-std 0.03 --plot\n</code></pre> <p>Sweeps over several hidden-layer configurations, printing accuracies and (optionally) plotting curves. Use <code>--scenario optimizer-compare</code> to benchmark SGD vs Adam; mix activations via <code>--hidden-activations relu,tanh,gelu</code>, tune learning-rate schedules via <code>--lr-schedule ...</code>, toggle richer augmentation via the <code>--augment-*</code> flags, and enable early stopping without editing code. Answer \"yes\" to the prompt before misclassification plotting (it triggers an extra pass).</p> <pre><code>python scripts/quickstart_convolutional.py --scenario gpu-fast --lookahead --plot\n</code></pre> <p>Trains ResNet18 with Lookahead + gradient clipping on GPU (falls back to CPU). Additional scenarios cover CPU baselines, checkpoint resume flows, and dataset swaps (e.g., CIFAR-10 shaped data via <code>--image-shape 3,32,32</code>). You will be prompted before the misclassification plots run; decline if you want to skip the extra inference sweep.</p> <p>Each script exposes flags (<code>--epochs</code>, <code>--batch-size</code>, dataset overrides, <code>--plot</code>, etc.) so students can experiment interactively without editing the main entrypoints.</p>"},{"location":"running-experiments/#advanced-training-recipes","title":"Advanced Training Recipes","text":"Goal Command Cosine LR + early stop on full MLP <code>python vectorized/main.py --epochs 15 --batch-size 128 --hidden-sizes 512,256,128 --hidden-activations relu,gelu,tanh --dropout 0.2 --batchnorm --lr-schedule cosine --min-lr 5e-5 --val-split 0.1 --early-stopping --early-patience 4 --early-delta 5e-4 --gpu</code> Plateau LR schedule via quick-start <code>python scripts/quickstart_vectorized.py --scenario optimizer-compare --epochs 6 --lr-schedule reduce_on_plateau --reduce-factor 0.4 --reduce-patience 2 --val-split 0.1 --early-stopping --plot</code> Compare stopping behavior across architectures <code>python scripts/quickstart_vectorized.py --scenario hidden-sweep --epochs 8 --hidden-options \"512,256;256,128,64\" --hidden-activations relu,gelu,tanh --dropout 0.2 --batchnorm --lr-schedule cosine --val-split 0.1 --early-stopping</code> Resume CNN run with scheduler <code>python convolutional/main.py resnet18 --epochs 3 --batch-size 128 --gpu --save checkpoints/resnet18_cosine.npz --lr-schedule cosine --lr-decay-min 1e-5</code> (then rerun with <code>--load checkpoints/resnet18_cosine.npz --epochs 2</code> to continue) <p>Tips:</p> <ul> <li>Watch the printed LR each epoch to confirm the schedule is working.</li> <li>Validation loss only appears when <code>--val-split &gt; 0</code> (or the quick-start scenario carries a validation set); early stopping relies on that signal.</li> <li>For rapid prototyping, drop <code>--epochs</code> to 2\u20133 and disable plotting. Once a schedule looks promising, bump epochs and re-enable visualizations.</li> </ul>"},{"location":"running-experiments/#suggested-experiments","title":"Suggested Experiments","text":"Idea What to measure CPU vs GPU timing Run the same architecture with and without <code>--gpu</code>. Record epoch time to quantify the hardware boost. Scalar vs Vectorized accuracy Train both implementations for one epoch and compare final accuracy to confirm mathematical equivalence. Architecture bake-off Train multiple CNNs for a fixed epoch budget and compare test accuracy vs parameter count. Hyperparameter tweaks Modify <code>--batch-size</code>, learning rate (edit optimizer), or augmentation toggle (<code>--no-augment</code>) to see their effect. Dropout/normalization ablations Temporarily disable layers (e.g., comment out BatchNorm) to witness training instability or overfitting. <p>Document your findings: timing tables, accuracy plots, or misclassification grids make great lab reports.</p>"},{"location":"running-experiments/#lab-challenges","title":"Lab Challenges","text":"<ol> <li> <p>Optimizer notebook: Use <code>scripts/quickstart_vectorized.py --scenario optimizer-compare --plot</code> to capture loss curves for SGD and Adam on the same dataset. Write a short paragraph explaining which optimizer converged faster and why.</p> </li> <li> <p>ResNet stress test: Run <code>python scripts/quickstart_convolutional.py --scenario gpu-fast --lookahead --plot</code> on a GPU (or CPU fallback). Measure runtime, GPU memory usage, and final accuracy, then describe one tweak that could reduce memory pressure.</p> </li> </ol>"},{"location":"running-experiments/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<ul> <li>CuPy import error: ensure CUDA toolkit + matching CuPy wheel are installed; run <code>python scripts/test_cupy.py</code> to confirm.</li> <li>FileNotFoundError for data: verify the <code>.npy</code> files exist under <code>data/</code>.</li> <li>Out-of-memory during misclassification plots: reduce the visualization batch size or run on CPU; the helper in <code>convolutional/main.py</code> already requests batched predictions, but huge grids can still consume memory.</li> <li>Long training time on large nets: start with <code>--epochs 1</code> to smoke-test your setup before longer runs.</li> </ul>"},{"location":"architectures/alexnet/","title":"AlexNet (2012)","text":"<p> Figure credit: Alex Krizhevsky et al., via Wikipedia (CC BY-SA 4.0).</p>"},{"location":"architectures/alexnet/#historical-context","title":"Historical Context","text":"<p>AlexNet, created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, won the 2012 ImageNet competition and sparked the modern deep-learning wave in computer vision. It proved that deep CNNs paired with GPUs could outperform traditional methods by a wide margin.</p>"},{"location":"architectures/alexnet/#architecture-highlights","title":"Architecture Highlights","text":"<ol> <li>Five convolutional layers with large early kernels (e.g., 11\u00d711) and overlapping max pooling.</li> <li>ReLU activations throughout\u2014faster to train than tanh/sigmoid.</li> <li>Dropout in the fully connected layers to combat overfitting.</li> <li>Data augmentation (random crops, flips) in the original paper; in this sandbox, shift augmentation covers similar ground.</li> </ol>"},{"location":"architectures/alexnet/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/alexnet.py</code></li> <li>Mimics the original filter sizes/channel counts but adapted to 28\u00d728 MNIST inputs (so spatial dimensions are smaller).</li> <li>Uses the shared Conv2D/BatchNorm/ReLU modules plus <code>Dense</code> layers with dropout in the head.</li> </ul>"},{"location":"architectures/alexnet/#teaching-angles","title":"Teaching Angles","text":"<ul> <li>Introduces deeper conv stacks and the idea of aggressively increasing feature channels.</li> <li>Shows how dropout became essential when networks grew larger.</li> <li>Good case study for GPU vs CPU timing\u2014AlexNet has enough parameters to reveal meaningful differences.</li> </ul>"},{"location":"architectures/alexnet/#suggested-experiments","title":"Suggested Experiments","text":"<ul> <li>Train for a few epochs on GPU and compare accuracy/time to LeNet.</li> <li>Toggle dropout or change batch size to study optimization stability.</li> </ul> <p>References</p> <ul> <li>AlexNet on Wikipedia</li> </ul>"},{"location":"architectures/baseline/","title":"Baseline CNN","text":""},{"location":"architectures/baseline/#why-it-exists","title":"Why It Exists","text":"<p>The BaselineCNN is a compact starter architecture built specifically for the sandbox. It bridges the gap between fully connected MNIST models and the more advanced historical networks. Its small size keeps iteration time low while still demonstrating convolution, pooling, and dropout in practice.</p>"},{"location":"architectures/baseline/#block-diagram","title":"Block Diagram","text":"<ol> <li>Conv \u2192 ReLU \u2192 Pool repeated three times with gradually increasing channel counts.</li> <li>Flatten \u2192 Dense \u2192 Dropout \u2192 Dense to map the extracted features to digit logits.</li> <li>Optional shift augmentation (handled in the trainer) to make the tiny network more robust.</li> </ol>"},{"location":"architectures/baseline/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/baseline.py</code></li> <li>Layers defined in: <code>convolutional/modules.py</code></li> <li>Uses standard 3\u00d73 kernels, stride 1, and 2\u00d72 max pooling to reduce spatial dimensions.</li> <li>Keeps parameter count low enough that CPU training is still quick, making it ideal for sanity checks or teaching sessions.</li> </ul>"},{"location":"architectures/baseline/#experiment-ideas","title":"Experiment Ideas","text":"<ul> <li>Compare BaselineCNN accuracy vs the vectorized MLP to highlight the benefits of convolutions on image data.</li> <li>Toggle dropout to illustrate overfitting on MNIST.</li> <li>Use it as a control when demonstrating CPU vs GPU speedups (short runs minimize waiting).</li> </ul>"},{"location":"architectures/baseline/#further-reading","title":"Further Reading","text":"<ul> <li>Review the scalar/vectorized tutorials first so you can inspect how convolutions extend the same ideas to 2D data.</li> </ul>"},{"location":"architectures/convnext-tiny/","title":"ConvNeXt-Tiny (2022)","text":"<p> Figure credit: Liu et al., via Wikipedia (CC BY 4.0).</p>"},{"location":"architectures/convnext-tiny/#historical-context","title":"Historical Context","text":"<p>Facebook AI\u2019s ConvNeXt revisits CNN design in the age of Vision Transformers. By adopting transformer-inspired choices\u2014large patchification, LayerNorm, GELU, depthwise separable convs\u2014the authors produced pure CNNs that rival transformer accuracy on ImageNet.</p>"},{"location":"architectures/convnext-tiny/#block-structure","title":"Block Structure","text":"<ol> <li>Patchify Stem: A large-stride convolution that acts like ViT\u2019s patch embedding.</li> <li>ConvNeXt Blocks: Each block applies a 7\u00d77 depthwise convolution, LayerNorm, an MLP-like pointwise expansion (ratio 4), GELU activation, and a residual connection.</li> <li>Downsampling between stages via stride-2 convolutions, similar to hierarchical transformers.</li> <li>Global average pooling + linear classifier at the end.</li> </ol>"},{"location":"architectures/convnext-tiny/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/convnext.py</code></li> <li>Uses <code>LayerNorm2D</code>, <code>GELU</code>, <code>DepthwiseConv2D</code>, and <code>GlobalAvgPool2D</code> from <code>convolutional/modules.py</code>.</li> <li>Keeps the Tiny variant\u2019s stage depths (3-3-9-3 blocks) adapted to MNIST input size.</li> </ul>"},{"location":"architectures/convnext-tiny/#teaching-angles","title":"Teaching Angles","text":"<ul> <li>Illustrates how modern CNNs borrow normalization and activation choices from transformers.</li> <li>Highlights the impact of large kernel depthwise convolutions for greater receptive fields.</li> <li>Shows the evolution from BatchNorm/Residual combos to LayerNorm/inverted bottlenecks.</li> </ul>"},{"location":"architectures/convnext-tiny/#suggested-experiments","title":"Suggested Experiments","text":"<ul> <li>Compare convergence speed and final accuracy vs ResNet18 to see how the newer design fares on MNIST.</li> <li>Visualize misclassifications: does ConvNeXt make different mistakes compared to classic nets?</li> <li>Try running just one stage (trim the architecture) to observe how depth affects performance.</li> </ul> <p>References</p> <ul> <li>ConvNeXt summary (Emergent Mind)</li> <li>\u201cA ConvNet for the 2020s\u201d paper</li> </ul>"},{"location":"architectures/efficientnet-lite0/","title":"EfficientNet-Lite0 (2019)","text":"<p> Figure credit: Tan &amp; Le, via Wikipedia (CC BY 4.0).</p>"},{"location":"architectures/efficientnet-lite0/#historical-context","title":"Historical Context","text":"<p>EfficientNet (Tan &amp; Le) introduced a family of models that scale depth, width, and input resolution in a balanced way. EfficientNet-Lite0 adapts those ideas for mobile/CPU targets by removing swish activations and float16, yet still delivering strong accuracy per parameter.</p>"},{"location":"architectures/efficientnet-lite0/#core-building-blocks","title":"Core Building Blocks","text":"<ul> <li>MBConv (Mobile Inverted Bottleneck): 1\u00d71 pointwise expansion \u2192 depthwise 3\u00d73 convolution \u2192 squeeze-and-excitation (SE) block \u2192 1\u00d71 projection.</li> <li>Squeeze-and-Excitation: Channel attention mechanism that globally pools each channel, passes through a small MLP, and scales the feature map.</li> <li>Depthwise convolutions: Provide most of the spatial processing at a fraction of the cost of full convolutions.</li> </ul>"},{"location":"architectures/efficientnet-lite0/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/efficientnet.py</code></li> <li>Includes helper classes for MBConv and SE directly within the architecture file.</li> <li>Uses <code>SiLU</code> activations for non-linearity and <code>Dropout</code> toward the end, mirroring the original paper\u2019s regularization.</li> </ul>"},{"location":"architectures/efficientnet-lite0/#teaching-angles","title":"Teaching Angles","text":"<ul> <li>Shows how careful block design can slash parameter counts without losing accuracy.</li> <li>Highlights compound scaling: you can imagine varying block repeats/channel widths to create the rest of the EfficientNet family.</li> <li>Introduces SE attention as a practical example of channel recalibration.</li> </ul>"},{"location":"architectures/efficientnet-lite0/#suggested-experiments","title":"Suggested Experiments","text":"<ul> <li>Compare accuracy vs VGG16 or ResNet18 for the same epoch budget to highlight efficiency.</li> <li>Inspect per-layer parameter counts using <code>sum(p[0].size for p in model.parameters())</code>.</li> <li>Demonstrate the impact of SE by temporarily disabling the squeeze-excite block (for exploratory learning).</li> </ul> <p>References</p> <ul> <li>EfficientNet-Lite overview (Luxonis)</li> <li>\u201cEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u201d</li> </ul>"},{"location":"architectures/lenet/","title":"LeNet-5 (1998)","text":"<p> Figure credit: Yann LeCun et al., via Wikipedia (CC BY-SA 3.0).</p>"},{"location":"architectures/lenet/#historical-context","title":"Historical Context","text":"<p>LeNet-5, introduced by Yann LeCun and colleagues, is one of the first successful convolutional neural networks. It was designed for handwritten digit recognition on bank checks and showed that local receptive fields plus pooling dramatically outperform dense networks on images.</p>"},{"location":"architectures/lenet/#architecture-overview","title":"Architecture Overview","text":"<ol> <li>Conv Layer 1: 6 filters of size 5\u00d75, followed by tanh/ReLU and 2\u00d72 subsampling.</li> <li>Conv Layer 2: 16 filters, again followed by subsampling.</li> <li>Fully Connected Stack: Two dense layers leading to a 10-way output with softmax.</li> </ol> <p>In this project, the implementation mirrors the original layout but uses ReLU activations for better gradient flow.</p>"},{"location":"architectures/lenet/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/lenet.py</code></li> <li>Layers: All building blocks come from <code>convolutional/modules.py</code>.</li> <li>Padding is used to maintain spatial dimensions before pooling, keeping the math close to the canonical LeNet.</li> </ul>"},{"location":"architectures/lenet/#teaching-angles","title":"Teaching Angles","text":"<ul> <li>Demonstrates how convolutions reduce parameter counts compared to dense layers.</li> <li>Illustrates early use of pooling (subsampling) for translational invariance.</li> <li>Perfect baseline for MNIST\u2014students can read the entire architecture in a few minutes.</li> </ul>"},{"location":"architectures/lenet/#suggested-experiments","title":"Suggested Experiments","text":"<ul> <li>Train for 1\u20132 epochs with and without augmentation to see robustness changes.</li> <li>Compare training time vs BaselineCNN or modern nets to emphasize efficiency.</li> </ul> <p>References</p> <ul> <li>LeNet on Wikipedia</li> </ul>"},{"location":"architectures/resnet18/","title":"ResNet-18 (2015)","text":"<p> Figure credit: He et al., via Wikipedia (CC BY 4.0).</p>"},{"location":"architectures/resnet18/#historical-context","title":"Historical Context","text":"<p>Residual Networks (ResNets) by He et al. solved the \u201cdegradation\u201d problem: deeper models were underperforming shallower ones because gradients struggled to flow through many layers. ResNets introduced identity skip connections that let layers learn residual functions, enabling networks with 100+ layers.</p>"},{"location":"architectures/resnet18/#architecture-structure","title":"Architecture Structure","text":"<ul> <li>Stem: 7\u00d77 Conv + MaxPool to quickly reduce spatial size (adapted for MNIST in this repo).</li> <li>Residual Stages: Four stages, each containing multiple residual blocks. Every block has two 3\u00d73 convolutions plus a skip path (identity or 1\u00d71 projection when dimensions change).</li> <li>Head: Global average pooling \u2192 fully connected classifier.</li> </ul>"},{"location":"architectures/resnet18/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/resnet.py</code></li> <li>Residual blocks live alongside helper modules in the same file; they rely on <code>BatchNorm2D</code>, <code>ReLU</code>, and <code>Conv2D</code> from <code>convolutional/modules.py</code>.</li> <li>Downsampling blocks use stride 2 plus a 1\u00d71 conv on the skip path to keep tensor shapes compatible.</li> </ul>"},{"location":"architectures/resnet18/#teaching-angles","title":"Teaching Angles","text":"<ul> <li>Demonstrates how skip connections preserve gradients and encourage feature reuse.</li> <li>Highlights the difference between \u201cbasic blocks\u201d (used here) and \u201cbottleneck blocks\u201d in deeper ResNets.</li> <li>Provides a concrete example of identity vs projection shortcuts.</li> </ul>"},{"location":"architectures/resnet18/#suggested-experiments","title":"Suggested Experiments","text":"<ul> <li>Comment out the skip addition (for a single block) to see training degrade\u2014great for illustrating why residuals matter.</li> <li>Compare training curves to VGG16; ResNet should converge faster/stabler despite similar depth.</li> </ul> <p>References</p> <ul> <li>\u201cDeep Residual Learning for Image Recognition\u201d</li> <li>Analytics Vidhya summary of skip connections</li> </ul>"},{"location":"architectures/vgg16/","title":"Vgg16","text":"<p>Figure credit: Simonyan &amp; Zisserman, via Wikipedia (CC BY 4.0).</p>"},{"location":"architectures/vgg16/#historical-context","title":"Historical Context","text":"<p>K. Simonyan and A. Zisserman\u2019s VGG-16 demonstrated that a deep network built from simple, uniform 3\u00d73 convolutions could reach top-tier accuracy on ImageNet. Its clean design made it a favorite starting point for transfer learning and architectural experiments.</p>"},{"location":"architectures/vgg16/#architecture-structure","title":"Architecture Structure","text":"<ul> <li>Convolutional Blocks: Five stages, each stacking 2\u20133 Conv2D layers with 3\u00d73 kernels, stride 1, and padding 1, followed by max pooling.</li> <li>Fully Connected Head: Two 4096-unit dense layers with ReLU + Dropout, then a final classifier layer.</li> <li>Depth: 13 convolutional layers + 3 dense layers = 16 learnable layers.</li> </ul>"},{"location":"architectures/vgg16/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>File: <code>convolutional/architectures/vgg.py</code></li> <li>The sandbox version adapts pooling strides to ensure the flattened size matches the dense layers (see recent fix for stride-1 pools near the end).</li> <li>ReLU activations and dropout mirror the original training recipe.</li> </ul>"},{"location":"architectures/vgg16/#teaching-angles","title":"Teaching Angles","text":"<ul> <li>Emphasizes the impact of depth and uniform design patterns.</li> <li>Highlights how quickly parameter counts explode (great example for discussing memory constraints).</li> <li>Serves as a gateway to discussions on why later models introduce residual connections or depthwise separable convs.</li> </ul>"},{"location":"architectures/vgg16/#suggested-experiments","title":"Suggested Experiments","text":"<ul> <li>Monitor GPU memory usage vs smaller architectures.</li> <li>Train with and without dropout to see overfitting on MNIST.</li> <li>Time a single epoch on CPU vs GPU to quantify acceleration benefits.</li> </ul> <p>References</p> <ul> <li>\u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d</li> <li>VGG16 overview (Great Learning blog)</li> </ul>"},{"location":"augmentations/cutmix-and-randaugment/","title":"CutMix & RandAugment","text":"<p>Advanced policies such as CutMix and RandAugment are now first-class citizens inside <code>common/augment.py</code>. This page explains how to configure and validate them.</p>"},{"location":"augmentations/cutmix-and-randaugment/#cutmix","title":"CutMix","text":"<ul> <li>Flags: <code>--augment-cutmix-prob</code>, <code>--augment-cutmix-alpha</code>.</li> <li>Logic:</li> <li>Sample two images and draw a random rectangle sized according to <code>lambda ~ Beta(alpha, alpha)</code>.</li> <li>Copy the rectangle from the second image into the first.</li> <li>Mix labels using <code>\u03bb * y_a + (1-\u03bb) * y_b</code>. Trainers compute a weighted sum of losses/gradients automatically.</li> <li>The scalar trainer automatically disables CutMix (<code>allow_label_mix=False</code>) to keep its simple per-sample label flow intact.</li> </ul> <p>Vectorized trainer example:</p> <pre><code>python3 vectorized/main.py --epochs 3 --batch-size 64 \\\n  --augment-cutmix-prob 0.4 --augment-cutmix-alpha 0.7 \\\n  --augment-noise-std 0.02 --augment-cutout-prob 0.2\n</code></pre> <p>Convolutional trainer example:</p> <pre><code>python3 convolutional/main.py resnet18 --epochs 3 --batch-size 64 \\\n  --augment-cutmix-prob 0.35 --augment-cutmix-alpha 1.0 \\\n  --augment-randaug-layers 1 --augment-randaug-magnitude 0.4\n</code></pre> <p>Validation tips:</p> <ul> <li>Log both <code>\u03bb</code> values and sample visualizations occasionally to ensure the bounding boxes make sense.</li> <li>Combine with <code>--show-misclassified</code> to inspect whether CutMix is making labels ambiguous.</li> </ul>"},{"location":"augmentations/cutmix-and-randaugment/#randaugment","title":"RandAugment","text":"<ul> <li>Flags: <code>--augment-randaug-layers</code>, <code>--augment-randaug-magnitude</code>.</li> <li>Implementation:</li> <li>For each layer, pick one op from <code>{rotate, shift_x, shift_y, flip_lr, flip_ud, noise, cutout}</code> at random.</li> <li>Apply it using the provided magnitude scaled into the op\u2019s natural units (degrees, pixels, probability, etc.).</li> <li>Works on all trainers; magnitude \u2208 [0, 1].</li> </ul> <p>Cookbook command (shared policy across stacks):</p> <pre><code>python3 scalar/main.py --epochs 2 --batch-size 64 \\\n  --augment-randaug-layers 1 --augment-randaug-magnitude 0.4 --augment-cutout-prob 0.2\n\npython3 vectorized/main.py --epochs 2 --batch-size 64 \\\n  --augment-randaug-layers 2 --augment-randaug-magnitude 0.5 \\\n  --augment-cutmix-prob 0.3 --augment-cutmix-alpha 0.9\n\npython3 convolutional/main.py baseline --epochs 2 --batch-size 64 \\\n  --augment-randaug-layers 2 --augment-randaug-magnitude 0.5\n</code></pre> <p>Practical advice:</p> <ol> <li>Start with one RandAugment layer on MNIST-class tasks; more layers may over-randomize digits.</li> <li>Combine with deterministic flags (e.g., keep <code>--augment-rotate-deg</code> low while RandAugment explores additional rotations).</li> <li>Always benchmark against <code>--no-augment</code> to quantify the gain.</li> </ol>"},{"location":"augmentations/geometry/","title":"Geometric Augmentations","text":"<p>This page covers the spatial transforms supported by <code>common/augment.py</code>: pixel shifts, rotations, and flips.</p>"},{"location":"augmentations/geometry/#pixel-shifts","title":"Pixel Shifts","text":"<ul> <li>Controlled via <code>--augment-max-shift</code> (integer radius).  </li> <li>Images are rolled along height/width before any other op.</li> <li>Shifts are label-safe for centered digits and general-purpose image datasets.</li> </ul> <p>Example:</p> <pre><code>python3 vectorized/main.py --epochs 1 --batch-size 64 \\\n  --augment-max-shift 3 --augment-rotate-deg 0 \\\n  --augment-hflip-prob 0 --augment-vflip-prob 0\n</code></pre>"},{"location":"augmentations/geometry/#rotations","title":"Rotations","text":"<ul> <li><code>--augment-rotate-deg</code> sets the maximum absolute angle.</li> <li><code>--augment-rotate-prob</code> gates how often we rotate.</li> <li>The shared helper uses nearest-neighbor sampling per channel and works on CPU or GPU transparently.</li> </ul> <p>Validation tip: start with small angles (\u226412\u00b0) on MNIST to avoid turning a \u201c6\u201d into a \u201c9\u201d. When experimenting with natural images, combine rotations with flips to simulate viewpoint changes.</p> <p>CLI example across all trainers:</p> <pre><code>python3 scalar/main.py --epochs 1 --batch-size 64 \\\n  --augment-rotate-deg 12 --augment-rotate-prob 0.7 \\\n  --augment-max-shift 2 --augment-hflip-prob 0\n\npython3 vectorized/main.py --epochs 1 --batch-size 64 \\\n  --augment-rotate-deg 15 --augment-rotate-prob 0.5\n\npython3 convolutional/main.py baseline --epochs 1 --batch-size 64 \\\n  --augment-rotate-deg 20 --augment-rotate-prob 0.5\n</code></pre>"},{"location":"augmentations/geometry/#flips","title":"Flips","text":"<ul> <li><code>--augment-hflip-prob</code> mirrors across the vertical axis.</li> <li><code>--augment-vflip-prob</code> mirrors across the horizontal axis.</li> <li>Useful for natural images (CIFAR, ImageNet) where mirrored scenes keep the same label.</li> <li>Digits caution: flipping can change semantics (mirrored \u201c2\u201d isn\u2019t a valid \u201c2\u201d). For MNIST, keep flip probabilities at 0 unless running an ablation.</li> </ul> <p>Mixed example (safe for CIFAR-style data):</p> <pre><code>python3 convolutional/main.py resnet18 --epochs 2 --batch-size 128 \\\n  --augment-hflip-prob 0.5 --augment-vflip-prob 0.1 \\\n  --augment-rotate-deg 5 --augment-rotate-prob 0.3\n</code></pre> <p>Remember to visualize augmented samples using the plotting utilities before committing to a policy.</p>"},{"location":"augmentations/noise-and-cutout/","title":"Noise & Cutout","text":"<p>Two powerful regularizers live in <code>common/augment.py</code>: additive Gaussian noise and cutout masks. Both are dataset-friendly when tuned carefully.</p>"},{"location":"augmentations/noise-and-cutout/#gaussian-noise","title":"Gaussian Noise","text":"<ul> <li><code>--augment-noise-std</code>: standard deviation of the noise sample.  </li> <li><code>--augment-noise-prob</code>: probability of applying noise per image.  </li> <li><code>--augment-noise-clip</code>: clamp range after noise injection (helps when data are standardized).</li> </ul> <p>Sample run (vectorized trainer):</p> <pre><code>python3 vectorized/main.py --epochs 1 --batch-size 64 \\\n  --augment-noise-std 0.04 --augment-noise-prob 0.6 \\\n  --augment-noise-clip 3.0\n</code></pre> <p>Tips:</p> <ul> <li>Standardize inputs first (the default <code>DataUtility.load_data()</code> already zero-centers and scales MNIST images).</li> <li>Use <code>plot_loss</code> to monitor whether noise slows convergence; reduce <code>noise_prob</code> if the model struggles early on.</li> </ul>"},{"location":"augmentations/noise-and-cutout/#cutout","title":"Cutout","text":"<ul> <li><code>--augment-cutout-prob</code> toggles random occlusion per sample.  </li> <li><code>--augment-cutout-size</code> measures the square patch length in pixels.  </li> <li>The helper zeros out a square centered at a random coordinate, clamped to image boundaries.</li> </ul> <p>Scalar trainer example:</p> <pre><code>python3 scalar/main.py --epochs 2 --batch-size 64 \\\n  --augment-cutout-prob 0.3 --augment-cutout-size 5 \\\n  --augment-noise-std 0.03 --augment-noise-prob 0.5\n</code></pre> <p>Convolutional trainer example (using both digits-safe settings):</p> <pre><code>python3 convolutional/main.py baseline --epochs 2 --batch-size 64 \\\n  --augment-cutout-prob 0.25 --augment-cutout-size 4 \\\n  --augment-noise-std 0.02 --augment-noise-prob 0.4\n</code></pre> <p>Practical advice:</p> <ol> <li>Start with small cutout sizes (3\u20135 pixels for MNIST) so digits remain identifiable.</li> <li>When training on larger datasets (e.g., CIFAR-10), gradually increase both size and probability.</li> <li>Combine with <code>--no-augment</code> baseline runs to quantify the benefit in your experiment logs.</li> </ol>"},{"location":"augmentations/overview/","title":"Augmentation Overview","text":"<p>All three training stacks (scalar, vectorized, convolutional) now consume a shared configuration from <code>common/augment.py</code>. That means every CLI exposes the same switches\u2014once you learn a flag, you can use it everywhere.</p>"},{"location":"augmentations/overview/#core-flags","title":"Core Flags","text":"Flag Meaning Notes <code>--augment-max-shift</code> Pixel jitter radius Applies before other ops. <code>--augment-rotate-deg</code> / <code>--augment-rotate-prob</code> Max rotation + probability Small angles preserve MNIST semantics. <code>--augment-hflip-prob</code> / <code>--augment-vflip-prob</code> Flip probabilities Use carefully for digits. <code>--augment-noise-std</code> / <code>--augment-noise-prob</code> / <code>--augment-noise-clip</code> Additive Gaussian noise Works best on normalized data. <code>--augment-cutout-prob</code> / <code>--augment-cutout-size</code> Random occlusion masks Caution on small digits. <code>--augment-cutmix-prob</code> / <code>--augment-cutmix-alpha</code> CutMix patches + label mixing Automatically disabled by the scalar trainer. <code>--augment-randaug-layers</code> / <code>--augment-randaug-magnitude</code> RandAugment policy layers Randomly stacks ops using the shared magnitude. <code>--no-augment</code> Disables everything Handy for ablations/tests."},{"location":"augmentations/overview/#one-liner-cookbook","title":"One-Liner Cookbook","text":"<pre><code># Scalar (no CutMix, but full geometric/noise stack)\npython3 scalar/main.py --epochs 2 --batch-size 64 \\\n  --augment-rotate-deg 12 --augment-rotate-prob 0.6 \\\n  --augment-cutout-prob 0.25 --augment-cutout-size 5 \\\n  --augment-randaug-layers 1 --augment-randaug-magnitude 0.4\n\n# Vectorized MLP with CutMix + RandAugment\npython3 vectorized/main.py --epochs 2 --batch-size 64 \\\n  --augment-cutmix-prob 0.3 --augment-cutmix-alpha 0.8 \\\n  --augment-randaug-layers 2 --augment-randaug-magnitude 0.5 \\\n  --augment-noise-std 0.03 --augment-cutout-prob 0.3\n\n# Convolutional trainer (baseline CNN)\npython3 convolutional/main.py baseline --epochs 2 --batch-size 64 \\\n  --augment-hflip-prob 0.4 --augment-cutmix-prob 0.3 \\\n  --augment-randaug-layers 2 --augment-randaug-magnitude 0.5\n</code></pre> <p>Each command exercises the exact same augmentation backend; only the model/optimizer stacks differ.</p>"},{"location":"augmentations/overview/#suggested-reading","title":"Suggested Reading","text":"<ul> <li>Geometry Ops (shift/rotate/flip)</li> <li>Noise + Cutout</li> <li>CutMix + RandAugment</li> </ul>"},{"location":"concepts/activations/","title":"Activation Functions","text":"<p>Activations introduce non-linearity so networks can model complex decision boundaries. This project implements several classics and modern favorites. All activation modules live in <code>convolutional/modules.py</code>, and simpler scalar versions appear in <code>scalar/neuron.py</code>.</p>"},{"location":"concepts/activations/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<ul> <li>Definition: <code>ReLU(x) = max(0, x)</code></li> <li>Derivative: 1 for <code>x &gt; 0</code>, 0 otherwise.</li> <li>Use in project: Default activation in most conv/dense layers. Found in BaselineCNN, LeNet, AlexNet, VGG, and ResNet blocks.</li> <li>Why it matters: Keeps gradients alive for positive activations, combats vanishing gradients, cheap to compute.</li> </ul>"},{"location":"concepts/activations/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li>Definition: <code>max(\u03b1x, x)</code> with a small \u03b1 (e.g., 0.01).</li> <li>Use: Provided for experimentation; prevents neurons from dying by allowing a tiny gradient for <code>x &lt; 0</code>.</li> </ul>"},{"location":"concepts/activations/#silu-swish","title":"SiLU (Swish)","text":"<ul> <li>Definition: <code>x \u00b7 sigmoid(x)</code></li> <li>Where used: EfficientNet-Lite0 MBConv blocks.</li> <li>Benefit: Smooth, non-monotonic activation that empirically improves performance in lightweight models.</li> </ul>"},{"location":"concepts/activations/#gelu-gaussian-error-linear-unit","title":"GELU (Gaussian Error Linear Unit)","text":"<ul> <li>Definition: <code>0.5 x (1 + tanh(\u221a(2/\u03c0)(x + 0.044715x^3)))</code></li> <li>Where used: ConvNeXt blocks, mirroring transformer activations.</li> <li>Reason: Provides smoother transitions than ReLU and works well with LayerNorm-based architectures.</li> </ul>"},{"location":"concepts/activations/#softmax","title":"Softmax","text":"<ul> <li>File: <code>common/softmax.py</code></li> <li>Purpose: Converts logits into probabilities for multi-class classifiers.</li> <li>Usage: Final layer in MLPs and CNNs before cross-entropy loss.</li> </ul>"},{"location":"concepts/activations/#activation-aware-initialization-cli-hooks","title":"Activation-Aware Initialization &amp; CLI Hooks","text":"<ul> <li>Files: <code>scalar/neuron.py</code>, <code>vectorized/modules.py</code> (<code>class Linear</code>)</li> <li>Heuristics: Hidden layers automatically switch between He initialization (ReLU, LeakyReLU, GELU) and Xavier/Glorot initialization (tanh, sigmoid) based on the activation you assign.</li> <li>How to control activations: Pass <code>--hidden-activations</code> to <code>scalar/main.py</code>, <code>vectorized/main.py</code>, or their quick-start scripts. Provide a comma-separated list (e.g., <code>relu,gelu,tanh</code>) that matches the number of hidden layers; the code applies the right initializer and derivative for each layer.</li> <li>Mix-and-match demos: Combine with <code>--leaky-negative-slope</code> (vectorized) to experiment with different slopes, or set one layer to <code>tanh</code> while others stay <code>relu</code> to observe convergence differences without touching the source files.</li> </ul>"},{"location":"concepts/activations/#study-checklist","title":"Study Checklist","text":"<ol> <li>Trace how each activation is implemented in <code>convolutional/modules.py</code>.</li> <li>Replace ReLU with another activation in a small model and observe training behavior.</li> <li>Compare gradient distributions (using prints or histograms) to see why smooth activations can aid optimization.</li> </ol>"},{"location":"concepts/backend/","title":"Backend & Device Utilities","text":"<p>The sandbox abstracts CPU and GPU execution through a lightweight backend layer so the same code can run on either NumPy or CuPy. Understanding this layer helps you debug device issues and write backend-agnostic modules.</p>"},{"location":"concepts/backend/#xp-proxy","title":"XP Proxy","text":"<ul> <li>File: <code>common/backend.py</code></li> <li>Class: <code>_XPProxy</code></li> <li>Behavior: Routes attribute access (<code>xp.exp</code>, <code>xp.zeros</code>) to the currently active array module (<code>numpy</code> by default, <code>cupy</code> when GPU is enabled).</li> <li>Usage: All modules import <code>xp = backend.xp</code> so they don\u2019t need to care about the underlying device.</li> </ul>"},{"location":"concepts/backend/#switching-devices","title":"Switching Devices","text":"<ul> <li><code>backend.use_gpu()</code> sets <code>xp = cupy</code> and marks <code>_using_gpu = True</code>.</li> <li><code>backend.use_cpu()</code> reverts back to NumPy.</li> <li><code>convolutional/main.py --gpu</code> calls <code>backend.use_gpu()</code> automatically, with a fallback if CuPy is unavailable.</li> </ul>"},{"location":"concepts/backend/#helper-functions","title":"Helper Functions","text":"<ul> <li><code>to_device(array, dtype=None)</code>: Converts arrays to the active backend, optionally casting to <code>dtype</code>.</li> <li><code>to_cpu(array)</code>: Ensures the output is a NumPy array (used for logging, plotting, checkpointing).</li> <li><code>get_array_module(array)</code>: Returns <code>numpy</code> or <code>cupy</code> depending on the array type\u2014useful when writing utilities that accept either.</li> </ul>"},{"location":"concepts/backend/#memory-safety","title":"Memory Safety","text":"<ul> <li>Optimizers and modules allocate state (e.g., zeros-like arrays) via <code>xp.zeros_like</code>, ensuring buffers live on the right device.</li> <li>Always avoid mixing NumPy and CuPy arrays in arithmetic; use <code>backend.to_device</code> and <code>backend.to_cpu</code> to convert explicitly.</li> </ul>"},{"location":"concepts/backend/#diagnostics","title":"Diagnostics","text":"<ul> <li><code>scripts/test_cupy.py</code> exercises allocations, kernels, matmuls, and custom elementwise code. Run it whenever you change CUDA drivers or hardware.</li> <li>If you see <code>cupy.cuda.memory.OutOfMemoryError</code>, reduce batch sizes or enable batching in visualization functions (already done for misclassification plots).</li> </ul>"},{"location":"concepts/backend/#best-practices","title":"Best Practices","text":"<ul> <li>Import <code>backend</code> once per module to avoid circular dependencies.</li> <li>Keep CPU-only utilities (like plotting) wrapped with <code>backend.to_cpu</code> so they don\u2019t accidentally receive CuPy arrays.</li> <li>When writing new modules, test them with both <code>backend.use_cpu()</code> and <code>backend.use_gpu()</code> to ensure portability.</li> </ul>"},{"location":"concepts/convolution/","title":"Convolution Mechanics & Advanced Blocks","text":"<p>Convolutional layers are the backbone of the CNN architectures in this sandbox. This note explains how they\u2019re implemented via <code>im2col</code>, and how advanced blocks build on top of them.</p>"},{"location":"concepts/convolution/#conv2d-via-im2col","title":"Conv2D via im2col","text":"<ul> <li>File: <code>convolutional/modules.py</code> (<code>class Conv2D</code>)</li> <li>Key functions: <code>_im2col</code> and <code>_col2im</code></li> <li>Process:</li> <li>Pad the input tensor (if needed).</li> <li>Extract sliding windows (patches) using <code>sliding_window_view</code> or manual strides.</li> <li>Reshape patches into a 2D matrix (<code>cols</code>) where each row corresponds to one receptive field.</li> <li>Flatten filters into a matrix (<code>W_col</code>) and perform a dense matrix multiply.</li> <li>Reshape the result back to <code>(N, out_channels, H_out, W_out)</code>.</li> <li>Backward pass: Uses <code>_col2im</code> to scatter gradients from the matrix form back into the original spatial layout.</li> </ul>"},{"location":"concepts/convolution/#depthwise-convolution","title":"Depthwise Convolution","text":"<ul> <li>File: <code>convolutional/modules.py</code> (<code>class DepthwiseConv2D</code>)</li> <li>Processes each channel independently, drastically reducing compute.</li> <li>Used in EfficientNet-Lite0 and ConvNeXt blocks as part of MBConv/inverted bottlenecks.</li> </ul>"},{"location":"concepts/convolution/#squeeze-and-excitation-se","title":"Squeeze-and-Excitation (SE)","text":"<ul> <li>File: <code>convolutional/modules.py</code> (<code>class SqueezeExcite</code>)</li> <li>Steps: global average pool \u2192 small MLP (reduce-then-expand) \u2192 sigmoid gating per channel.</li> <li>Purpose: Reweight feature maps so important channels are amplified.</li> </ul>"},{"location":"concepts/convolution/#global-average-pooling","title":"Global Average Pooling","text":"<ul> <li>File: <code>convolutional/modules.py</code> (<code>class GlobalAvgPool2D</code>)</li> <li>Replaces large fully connected heads by averaging each channel over spatial dimensions.</li> <li>Used in ResNet, EfficientNet, and ConvNeXt classifiers.</li> </ul>"},{"location":"concepts/convolution/#putting-it-together","title":"Putting It Together","text":"<ul> <li>MBConv (EfficientNet): 1\u00d71 expansion \u2192 depthwise conv \u2192 SE \u2192 1\u00d71 projection, plus residual if shapes match.</li> <li>ConvNeXt block: 7\u00d77 depthwise conv \u2192 LayerNorm \u2192 pointwise expansion (ratio 4) \u2192 GELU \u2192 pointwise projection \u2192 residual add.</li> </ul>"},{"location":"concepts/convolution/#exercises","title":"Exercises","text":"<ol> <li>Inspect <code>_im2col</code> and <code>_col2im</code> to understand how memory layout tricks speed up convolution.</li> <li>Modify kernel size or stride in <code>Conv2D</code> and verify the output dimensions using <code>_compute_output_dim</code>.</li> <li>Visualize SE weights to see which channels the network deems important for specific digits.</li> </ol>"},{"location":"concepts/fundamentals/","title":"Fundamentals: Neurons, Layers, Networks","text":"<p>Understanding neural networks starts with the simplest building block: a neuron that performs a weighted sum followed by a non-linearity. This section connects the theory to the code in the scalar and vectorized implementations.</p>"},{"location":"concepts/fundamentals/#neuron-mechanics","title":"Neuron Mechanics","text":"<ul> <li>File to inspect: <code>scalar/neuron.py</code></li> <li>Each neuron stores its weights, bias, and intermediate values (<code>z</code>, <code>activation</code>).</li> <li>Forward pass: <code>z = w \u00b7 x + b</code>, followed by an activation function (<code>sigmoid</code>, <code>tanh</code>, or ReLU in variants).</li> <li>Backward pass: Uses the chain rule to compute <code>\u2202L/\u2202w</code>, <code>\u2202L/\u2202b</code>, and <code>\u2202L/\u2202x</code>.</li> </ul>"},{"location":"concepts/fundamentals/#practice-tips","title":"Practice Tips","text":"<ol> <li>Run <code>python scalar/main.py</code> and print neuron activations to see how values change.</li> <li>Modify the activation function to confirm how gradients are affected.</li> </ol>"},{"location":"concepts/fundamentals/#layers-and-modules","title":"Layers and Modules","text":"<ul> <li>Scalar version: <code>scalar/layer.py</code> groups neurons and handles per-layer forward/backward logic.</li> <li>Vectorized version: <code>vectorized/modules.py</code> replaces explicit loops with matrix operations, but retains the same conceptual flow.</li> <li>Each layer maintains its own parameters and gradients, exposing them via <code>parameters()</code> so optimizers can update them.</li> </ul>"},{"location":"concepts/fundamentals/#assembling-networks","title":"Assembling Networks","text":"<ul> <li>Files: <code>scalar/network.py</code>, <code>vectorized/modules.py</code> (Sequential class), <code>convolutional/modules.py</code>.</li> <li>Networks are simply chains of modules. During a forward pass, tensors flow through each module in order; during backward, gradients traverse in reverse.</li> <li>This mirrors the mathematical composition of functions: <code>f(x) = f_n(...f_2(f_1(x)))</code>.</li> </ul>"},{"location":"concepts/fundamentals/#why-it-matters","title":"Why It Matters","text":"<ul> <li>A solid grasp of neurons/layers demystifies backpropagation. Once you understand the scalar implementation, every other architecture becomes a composition of the same primitives.</li> <li>Debugging tip: if a deep model misbehaves, temporarily swap in a smaller Sequential stack to isolate which module behaves unexpectedly.</li> </ul>"},{"location":"concepts/loss-and-softmax/","title":"Loss & Softmax","text":"<p>Classification networks in this repo use the cross-entropy loss applied directly to raw logits. This ensures numerical stability and pairs cleanly with the softmax function.</p>"},{"location":"concepts/loss-and-softmax/#where-to-look","title":"Where to Look","text":"<ul> <li>Primary file: <code>common/cross_entropy.py</code></li> <li>Helper: <code>common/softmax.py</code></li> <li>Trainer usage: <code>common/cross_entropy.CrossEntropyLoss</code> is instantiated inside <code>scalar/trainer.py</code>, <code>vectorized/trainer.py</code>, and <code>convolutional/trainer.py</code>.</li> </ul>"},{"location":"concepts/loss-and-softmax/#concept-recap","title":"Concept Recap","text":"<ul> <li>Softmax: <code>softmax(z)_i = exp(z_i) / \u03a3_j exp(z_j)</code> converts logits <code>z</code> into probabilities.</li> <li>Cross-Entropy: For target class <code>y</code>, <code>CE = -log(softmax(z)_y)</code>. It penalizes low probability assigned to the correct class.</li> </ul>"},{"location":"concepts/loss-and-softmax/#numerically-stable-implementation","title":"Numerically Stable Implementation","text":"<p>Directly computing softmax can overflow when logits are large. The code uses the log-sum-exp trick:</p> <ol> <li>Subtract the max logit from all logits to keep numbers small.</li> <li>Compute <code>lse = log(sum(exp(z - z_max)))</code>.</li> <li>Loss becomes <code>-z_y + z_max + lse</code>.</li> </ol>"},{"location":"concepts/loss-and-softmax/#targets-handling","title":"Targets Handling","text":"<ul> <li>Accepts either integer class indices or one-hot vectors.</li> <li>Converts everything to the active backend (<code>xp</code>) so CPU and GPU follow the same path.</li> </ul>"},{"location":"concepts/loss-and-softmax/#gradients","title":"Gradients","text":"<ul> <li>The gradient of cross-entropy with softmax simplifies to <code>softmax(z) - one_hot(target)</code>.</li> <li><code>cross_entropy_grad_logits</code> implements this efficiently, which is why trainers call <code>criterion.backward(logits, targets)</code> immediately after the forward pass.</li> </ul>"},{"location":"concepts/loss-and-softmax/#exercises","title":"Exercises","text":"<ol> <li>Print both logits and loss for a small batch to see how cross-entropy behaves as predictions improve.</li> <li>Manually compute the loss for a few samples and compare with the function output to build intuition.</li> <li>Experiment with label smoothing (modify targets slightly) to understand its regularization effect.</li> </ol>"},{"location":"concepts/normalization/","title":"Normalization Layers","text":"<p>Normalization stabilizes activations, speeds up training, and often improves generalization. This project provides both BatchNorm and LayerNorm variants tailored for CNNs.</p>"},{"location":"concepts/normalization/#batch-normalization-batchnorm2d","title":"Batch Normalization (BatchNorm2D)","text":"<ul> <li>File: <code>convolutional/modules.py</code> (<code>class BatchNorm2D</code>)</li> <li>Behavior: For each feature channel, computes batch mean/variance, normalizes inputs, then applies learnable scale (<code>gamma</code>) and shift (<code>beta</code>).</li> <li>Running stats: Maintains moving averages of mean/variance for inference mode.</li> <li>Where used: BaselineCNN, AlexNet, VGG16, ResNet18, EfficientNet blocks.</li> <li>Benefits: Allows higher learning rates, mitigates internal covariate shift, provides mild regularization.</li> </ul>"},{"location":"concepts/normalization/#layer-normalization-layernorm2d","title":"Layer Normalization (LayerNorm2D)","text":"<ul> <li>File: <code>convolutional/modules.py</code> (<code>class LayerNorm2D</code>)</li> <li>Behavior: Normalizes across feature channels within each sample, independent of batch size.</li> <li>Where used: ConvNeXt blocks, where transformer-style norms replace BatchNorm.</li> <li>Benefits: Stable even with small batch sizes and fits architectures inspired by transformers.</li> </ul>"},{"location":"concepts/normalization/#batchnorm-for-fully-connected-layers","title":"BatchNorm for Fully Connected Layers","text":"<ul> <li>File: <code>vectorized/modules.py</code> (<code>class BatchNorm1D</code>)</li> <li>Behavior: Mirrors the 2D version but operates on <code>[batch, features]</code> tensors, making it ideal for the fully connected MLP.</li> <li>How to enable: Run <code>python vectorized/main.py --batchnorm ...</code> (or add <code>--batchnorm</code> to the quick-start script). The flag inserts <code>BatchNorm1D</code> between the Linear layer and its activation for every hidden block. Adjust <code>--bn-momentum</code> to control how quickly running statistics adapt.</li> <li>Experiment: Train once with <code>--batchnorm</code> and once without, keeping <code>--dropout</code> constant. Compare convergence speed and test accuracy, especially with larger hidden sizes.</li> </ul>"},{"location":"concepts/normalization/#implementation-details","title":"Implementation Details","text":"<ul> <li>Both layers store intermediate state (<code>_cache</code>) for backward passes.</li> <li>During evaluation, they skip batch statistics and rely on stored parameters to keep inference deterministic.</li> <li>The trainer calls <code>model.train()</code> / <code>model.eval()</code> to switch behavior appropriately.</li> </ul>"},{"location":"concepts/normalization/#experiments","title":"Experiments","text":"<ol> <li>Disable BatchNorm in VGG16 (temporarily) to observe slower convergence or instability.</li> <li>Swap BatchNorm for LayerNorm in a block to see how it affects performance on small batches.</li> <li>Log running means/vars for BatchNorm to understand how they converge over time.</li> </ol>"},{"location":"concepts/optimizers/","title":"Optimizers","text":"<p>This hub links to dedicated tutorials for every optimization technique used in the sandbox. Each page covers the math, implementation details, CLI integration, and original references so you can understand why and how to tune training dynamics.</p>"},{"location":"concepts/optimizers/#available-tutorials","title":"Available Tutorials","text":"Topic Highlights Link Stochastic Gradient Descent Vanilla SGD, momentum, Nesterov updates, weight decay Learn SGD Adam Adaptive moments, bias correction, default hyperparameters Learn Adam Lookahead Slow/fast weight coupling, CLI flags (<code>--lookahead</code>) Learn Lookahead Gradient Clipping Why/when to clip, using <code>--grad-clip</code>, mathematical reasoning Learn gradient clipping <p>Use these references when you need to justify optimizer choices in lab reports or debug training behavior.</p>"},{"location":"concepts/regularization/","title":"Regularization & Augmentation","text":"<p>Regularization techniques keep models from overfitting and improve generalization. This sandbox implements several classic methods that you can toggle or modify.</p>"},{"location":"concepts/regularization/#dropout","title":"Dropout","text":"<ul> <li>Files: <code>scalar/layer.py</code> (<code>DropoutLayer</code>), <code>vectorized/modules.py</code> (<code>class Dropout</code>), <code>convolutional/modules.py</code>.</li> <li>Mechanism: During training, randomly zeroes activations with probability <code>p</code> and scales by <code>1 / (1 - p)</code> to keep expectations consistent.</li> <li>Usage: Added after every hidden layer in the scalar/vectorized MLPs when you pass <code>--hidden-dropout ...</code> or <code>--dropout ...</code> from the CLI; also used inside the CNN heads (BaselineCNN, AlexNet, VGG16, EfficientNet).</li> <li>Experiment: Train the vectorized MLP twice\u2014once with <code>--dropout 0</code> and once with <code>--dropout 0.3</code>\u2014to see how the loss curves diverge. Repeat in the scalar path with <code>--hidden-dropout 0.3,0.2,0.1</code> to match lecture exercises.</li> </ul>"},{"location":"concepts/regularization/#weight-decay-l2-regularization","title":"Weight Decay (L2 Regularization)","text":"<ul> <li>Implemented inside optimizers by adding <code>weight_decay * param</code> to gradients before updates.</li> <li>Shrinks weights toward zero and discourages overly complex solutions.</li> </ul>"},{"location":"concepts/regularization/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Function: <code>ConvTrainer._augment_batch</code> (in <code>convolutional/trainer.py</code>)</li> <li>Current augmentation: Random shifts up to \u00b12 pixels using <code>xp.roll</code>.</li> <li>Teaching point: Even simple translations can make MNIST models more robust; try increasing <code>max_shift</code> or adding random noise.</li> <li>New CLI controls: Vectorized and convolutional entry points now accept <code>--augment-max-shift</code>, <code>--augment-rotate-deg</code>, <code>--augment-hflip-prob</code>, and <code>--augment-noise-std</code> so you can enable rotations, flips, and noise injections from the command line instead of editing tutorial notebooks.</li> </ul>"},{"location":"concepts/regularization/#early-stopping-diy","title":"Early Stopping (DIY)","text":"<ul> <li>Not built-in, but you can monitor validation accuracy and halt training when it plateaus. Combine with checkpoints (<code>--save</code>) to resume later if needed.</li> </ul>"},{"location":"concepts/regularization/#batch-size-effects","title":"Batch Size Effects","text":"<ul> <li>Smaller batches introduce noise in gradient estimates\u2014this acts as a mild regularizer.</li> <li>Experiment by halving/doubling <code>--batch-size</code> and observing test accuracy variance.</li> </ul>"},{"location":"concepts/regularization/#checklist-for-labs","title":"Checklist for Labs","text":"<ol> <li>Run baseline without augmentation to measure overfitting.</li> <li>Enable dropout and augmentation; quantify the improvement.</li> <li>Plot weight histograms before and after applying weight decay to see shrinkage.</li> </ol>"},{"location":"optimizers/adam/","title":"Adam (Adaptive Moment Estimation)","text":"<p>Adam combines momentum and adaptive learning rates by keeping first and second moment estimates of gradients. Introduced by Kingma &amp; Ba (2014), it became the default optimizer for many deep-learning tasks thanks to its robustness and minimal tuning.</p>"},{"location":"optimizers/adam/#math-recap","title":"Math Recap","text":"<p>For gradient \\( g_t \\) at step \\( t \\):</p> <ol> <li>First moment (mean):    $$    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t    $$</li> <li>Second moment (uncentered variance):    $$    v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2    $$</li> <li>Bias correction:    $$    \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\qquad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}    $$</li> <li>Parameter update:    $$    w_{t+1} = w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}    $$</li> </ol> <p>Default hyperparameters: \\( \\beta_1 = 0.9 \\), \\( \\beta_2 = 0.999 \\), \\( \\epsilon = 10^{-8} \\).</p>"},{"location":"optimizers/adam/#implementation-in-the-sandbox","title":"Implementation in the Sandbox","text":"<ul> <li>File: <code>vectorized/optim.py</code>, class <code>Adam</code>.</li> <li>Stores per-parameter dictionaries <code>_m</code>, <code>_v</code>, and <code>_t</code>.</li> <li>Uses the backend proxy (<code>xp</code>) so it works on NumPy or CuPy tensors.</li> <li>Applies weight decay (if specified) before moment calculations.</li> </ul>"},{"location":"optimizers/adam/#usage-examples","title":"Usage Examples","text":"<p>Convolutional CLI (default):</p> <pre><code>optim = Adam(lr=5e-4, weight_decay=1e-4)\ntrainer = ConvTrainer(model, optim, num_classes=10, grad_clip_norm=args.grad_clip)\n</code></pre> <p>Vectorized MLP:</p> <pre><code>trainer = VTrainer(model, Adam(lr=1e-3), grad_clip_norm=1.0)\ntrainer.train(X_train, y_train, epochs=10, batch_size=128)\n</code></pre> <p>With Lookahead wrapper:</p> <pre><code>from vectorized.optim import Adam, Lookahead\noptim = Lookahead(Adam(lr=5e-4, weight_decay=1e-4), k=5, alpha=0.5)\n</code></pre>"},{"location":"optimizers/adam/#practical-guidance","title":"Practical Guidance","text":"<ul> <li>Adam is forgiving on learning rate; start with <code>1e-3</code> for MLPs and <code>5e-4</code> for CNNs, then fine-tune.</li> <li>Combine with <code>--grad-clip</code> if you observe spikes in loss (e.g., <code>--grad-clip 5</code>).</li> <li>When resuming from checkpoints, optimizer state (<code>m</code>, <code>v</code>, <code>t</code>) is restored via <code>common/model_io.py</code>, so you can continue training seamlessly.</li> </ul>"},{"location":"optimizers/adam/#reference","title":"Reference","text":"<ul> <li>Kingma, D. P., &amp; Ba, J. (2014). \u201cAdam: A Method for Stochastic Optimization.\u201d arXiv:1412.6980. https://arxiv.org/abs/1412.6980</li> </ul>"},{"location":"optimizers/gradient-clipping/","title":"Gradient Clipping","text":"<p>Gradient clipping limits the magnitude of backpropagated gradients to prevent exploding updates. Originally popularized in recurrent networks (Pascanu et al., 2013), it\u2019s also useful for CNNs and MLPs when experimenting with aggressive learning rates or new architectures.</p>"},{"location":"optimizers/gradient-clipping/#global-norm-clipping","title":"Global Norm Clipping","text":"<p>The sandbox implements global-norm clipping:</p> <ol> <li>Compute \\( \\|g\\| = \\sqrt{\\sum_i \\|g_i\\|^2} \\) across all parameter gradients.</li> <li>If \\( \\|g\\| &gt; c \\) (threshold), scale every gradient tensor by \\( c / (\\|g\\| + \\varepsilon) \\).</li> <li>Proceed with the optimizer step.</li> </ol> <p>This preserves gradient direction while shrinking its magnitude, keeping updates stable.</p>"},{"location":"optimizers/gradient-clipping/#implementation-details","title":"Implementation Details","text":"<ul> <li>Vectorized trainer: <code>_clip_gradients</code> method in <code>vectorized/trainer.py</code>.</li> <li>Convolutional trainer: <code>_clip_gradients</code> method in <code>convolutional/trainer.py</code>.</li> <li>Both take <code>grad_clip_norm</code> (float) at construction. When <code>None</code> or \u22640, clipping is disabled.</li> </ul>"},{"location":"optimizers/gradient-clipping/#cli-usage","title":"CLI Usage","text":"<p><code>convolutional/main.py</code> exposes the feature via <code>--grad-clip</code>:</p> <pre><code>python convolutional/main.py resnet18 --epochs 5 --batch-size 128 --gpu \\\n    --grad-clip 5.0\n</code></pre> <ul> <li>Set <code>--grad-clip</code> to the desired norm (e.g., <code>1.0</code>, <code>5.0</code>). Leave unset to disable.</li> <li>The value is passed to <code>ConvTrainer(..., grad_clip_norm=args.grad_clip)</code>.</li> </ul> <p>For vectorized runs, instantiate the trainer directly:</p> <pre><code>trainer = VTrainer(model, Adam(lr=1e-3), grad_clip_norm=1.0)\n</code></pre>"},{"location":"optimizers/gradient-clipping/#when-to-enable","title":"When to Enable","text":"<ul> <li>Loss spikes or diverges after a few batches.</li> <li>Training with high learning rates or on architectures newly added to the sandbox.</li> <li>Using optimizers without adaptive behavior (plain SGD) on deeper networks.</li> </ul>"},{"location":"optimizers/gradient-clipping/#tips","title":"Tips","text":"<ul> <li>Start with a moderate threshold (e.g., 5.0 for CNNs). Lower the value if you still observe instability.</li> <li>Combine with Lookahead to further stabilize training.</li> <li>Monitor gradient norms before/after clipping (add temporary prints) to understand how often clipping triggers.</li> </ul>"},{"location":"optimizers/gradient-clipping/#reference","title":"Reference","text":"<ul> <li>Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013). \u201cOn the difficulty of training recurrent neural networks.\u201d arXiv:1211.5063. https://arxiv.org/abs/1211.5063</li> </ul>"},{"location":"optimizers/lookahead/","title":"Lookahead Optimizer","text":"<p>Lookahead (Zhang et al., 2019) is a meta-optimizer that wraps any base optimizer (SGD, Adam, etc.) with a slow/fast weight coupling. It improves stability and often yields better generalization with little overhead.</p>"},{"location":"optimizers/lookahead/#idea-in-a-nutshell","title":"Idea in a Nutshell","text":"<ol> <li>Maintain two sets of weights: fast weights (updated every step by the base optimizer) and slow weights (periodically synchronized).</li> <li>After every \\( k \\) updates, interpolate:    [    w_{\\text{slow}} = w_{\\text{slow}} + \\alpha (w_{\\text{fast}} - w_{\\text{slow}})    ]    Then copy <code>w_slow</code> back into the fast weights.</li> <li>Fast weights continue training from the interpolated position.</li> </ol> <p>Parameters:</p> <ul> <li>\\( k \\): number of inner steps before synchronization (default 5).</li> <li>\\( \\alpha \\): interpolation factor (default 0.5).</li> </ul>"},{"location":"optimizers/lookahead/#implementation","title":"Implementation","text":"<ul> <li>File: <code>vectorized/optim.py</code>, class <code>Lookahead</code>.</li> <li>Wraps any optimizer inheriting from <code>Optimizer</code>.</li> <li>Stores a per-parameter \u201cslow\u201d copy in <code>_slow</code>.</li> <li>Calls <code>base_optimizer.step(...)</code> every iteration; applies synchronization when <code>step % k == 0</code>.</li> </ul>"},{"location":"optimizers/lookahead/#example-construction","title":"Example Construction","text":"<pre><code>from vectorized.optim import Adam, Lookahead\n\nbase = Adam(lr=5e-4, weight_decay=1e-4)\noptim = Lookahead(base, k=5, alpha=0.5)\ntrainer = ConvTrainer(model, optim, num_classes=10, grad_clip_norm=args.grad_clip)\n</code></pre>"},{"location":"optimizers/lookahead/#cli-support","title":"CLI Support","text":"<p><code>convolutional/main.py</code> exposes Lookahead through flags:</p> <pre><code>python convolutional/main.py resnet18 --epochs 5 --batch-size 128 --gpu \\\n    --lookahead --lookahead-k 5 --lookahead-alpha 0.5\n</code></pre> <ul> <li><code>--lookahead</code>: enables the wrapper.</li> <li><code>--lookahead-k</code>: number of inner updates.</li> <li><code>--lookahead-alpha</code>: interpolation factor.</li> </ul> <p>Vectorized MLPs currently require a code edit (wrap the optimizer manually as in the snippet above).</p>"},{"location":"optimizers/lookahead/#when-to-use","title":"When to Use","text":"<ul> <li>Training oscillates or diverges with the base optimizer alone.</li> <li>You want Adam\u2019s rapid progress but closer-to-SGD generalization.</li> <li>You\u2019re experimenting with higher learning rates and need extra stability.</li> </ul>"},{"location":"optimizers/lookahead/#reference","title":"Reference","text":"<ul> <li>Zhang, M., Lucas, J., Hinton, G., &amp; Ba, J. (2019). \u201cLookahead Optimizer: k steps forward, 1 step back.\u201d arXiv:1907.08610. https://arxiv.org/abs/1907.08610</li> </ul>"},{"location":"optimizers/sgd/","title":"Stochastic Gradient Descent (SGD) & Momentum","text":"<p>SGD is the workhorse of neural-network training. This note covers the math behind plain SGD, Polyak momentum, and Nesterov accelerated gradient (NAG), shows how they\u2019re implemented in this repo, and explains how to wire them into the CLI.</p>"},{"location":"optimizers/sgd/#core-update-rule","title":"Core Update Rule","text":"<p>Given parameters \\( w \\) and gradient estimate \\( g \\) (averaged over a mini-batch), vanilla SGD performs:</p> \\[ w_{t+1} = w_t - \\eta \\, g_t \\] <p>where \\( \\eta \\) is the learning rate. This idea was formalized by Robbins &amp; Monro (1951) in \u201cA Stochastic Approximation Method.\u201d</p>"},{"location":"optimizers/sgd/#momentum-polyak-1964","title":"Momentum (Polyak, 1964)","text":"<p>Momentum accumulates an exponential moving average of past gradients:</p> \\[ v_{t+1} = \\mu v_t + g_t, \\qquad w_{t+1} = w_t - \\eta v_{t+1} \\] <p>with momentum coefficient \\( \\mu \\in [0,1) \\). This \u201cheavy-ball\u201d method smooths updates and accelerates convergence along ravines.</p>"},{"location":"optimizers/sgd/#nesterov-accelerated-gradient-nag","title":"Nesterov Accelerated Gradient (NAG)","text":"<p>Nesterov (1983) proposed looking ahead before computing the gradient:</p> <ol> <li>\\( v_{t+1} = \\mu v_t + g(w_t - \\eta \\mu v_t) \\)</li> <li>\\( w_{t+1} = w_t - \\eta v_{t+1} \\)</li> </ol> <p>The code implements this by adding an extra term when <code>nesterov=True</code>.</p>"},{"location":"optimizers/sgd/#implementation-in-the-sandbox","title":"Implementation in the Sandbox","text":"<ul> <li>File: <code>vectorized/optim.py</code>, class <code>SGD</code>.</li> <li>Key features:</li> <li>Optional <code>momentum</code> and <code>nesterov</code> flags.</li> <li>Weight decay (<code>weight_decay * p</code>) added to gradients before applying updates.</li> <li>Uses <code>xp.zeros_like</code> so buffers live on CPU or GPU according to the backend.</li> </ul>"},{"location":"optimizers/sgd/#how-to-use","title":"How to Use","text":"<pre><code>from vectorized.optim import SGD\n\noptim = SGD(lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\ntrainer = ConvTrainer(model, optim, num_classes=10, grad_clip_norm=args.grad_clip)\n</code></pre> <p>Currently, the CLI defaults to Adam. To switch to SGD globally, edit <code>convolutional/main.py</code> and replace the optimizer construction with the snippet above (or make it conditional on a new CLI flag if desired).</p>"},{"location":"optimizers/sgd/#vectorized-mlp-example","title":"Vectorized MLP Example","text":"<pre><code>trainer = VTrainer(model, SGD(lr=0.05, momentum=0.9), grad_clip_norm=1.0)\ntrainer.train(X_train, y_train, epochs=20, batch_size=128)\n</code></pre>"},{"location":"optimizers/sgd/#practical-tips","title":"Practical Tips","text":"<ul> <li>Start with <code>lr=0.01</code> and <code>momentum=0.9</code> for CNNs on MNIST; adjust by factors of 3 if training diverges or converges too slowly.</li> <li>Combine with <code>--grad-clip</code> to keep updates stable at higher learning rates.</li> <li>Use the default learning-rate schedule (<code>ConvTrainer._default_multistep_schedule</code>) to drop the LR mid-training, mimicking classic ImageNet training recipes.</li> </ul> <p>References</p> <ul> <li>Robbins, H., &amp; Monro, S. (1951). \u201cA Stochastic Approximation Method.\u201d Annals of Mathematical Statistics. https://projecteuclid.org/journals/annals-of-mathematics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full</li> <li>Polyak, B. T. (1964). \u201cSome methods of speeding up the convergence of iteration methods.\u201d USSR Computational Mathematics and Mathematical Physics. https://doi.org/10.1016/0041-5553(64)90137-5</li> <li>Nesterov, Y. (1983). \u201cA method for solving the convex programming problem with convergence rate O(1/k^2).\u201d Soviet Mathematics Doklady.</li> </ul>"},{"location":"tools/","title":"Tools Overview","text":"<p>This section collects short references for the utility libraries used throughout the project. Jump directly to the page you need:</p> <ul> <li>NumPy \u2014 array operations, broadcasting, and tips for writing vectorized code.</li> <li>matplotlib \u2014 plotting utilities for visualizing samples and curves.</li> <li>tqdm \u2014 progress bars for long-running training loops.</li> <li>CuPy \u2014 GPU-backed NumPy equivalent used by the convolutional trainer.</li> </ul> <p>Use this hub when a notebook or script calls out one of the tools and you need a refresher.</p>"},{"location":"tools/cupy/","title":"Cupy","text":""},{"location":"tools/cupy/#why-you-need-cupy","title":"Why You Need CuPy","text":"<p>CuPy mirrors the NumPy API but executes on NVIDIA GPUs. By swapping the backend (<code>backend.use_gpu()</code> or <code>--gpu</code> flag), the project instantly accelerates heavy tensor ops without changing core logic.</p>"},{"location":"tools/cupy/#where-it-appears-in-the-project","title":"Where It Appears in the Project","text":"<ul> <li>Backend proxy: <code>common/backend.py</code> sets <code>xp = cupy</code> when you call <code>backend.use_gpu()</code> or pass <code>--gpu</code> to <code>convolutional/main.py</code>.</li> <li>Vectorized &amp; CNN modules: Every layer/optimizer referencing <code>xp</code> automatically runs on CuPy when available.</li> <li>Diagnostics: <code>scripts/test_cupy.py</code> ensures your drivers, CUDA runtime, and CuPy installation are healthy before long training jobs.</li> </ul>"},{"location":"tools/cupy/#setup-checklist","title":"Setup Checklist","text":"<ol> <li>Confirm that NVIDIA drivers + CUDA toolkit are installed (<code>nvidia-smi</code> should list your GPU).</li> <li>Install a CuPy build that matches your CUDA version (see below for commands).</li> <li>Verify with <code>python -c \"import cupy; print(cupy.__version__)\"</code>.</li> <li>Run <code>python scripts/test_cupy.py --stress-seconds 10 --stress-size 4096</code> to confirm kernels, memory pools, and custom ops behave.</li> </ol> <p>Once configured, add <code>--gpu</code> to any training command to feel the difference in throughput.</p>"},{"location":"tools/cupy/#installation-notes","title":"Installation Notes","text":"CUDA version Pip command Conda (conda-forge) CUDA 12.x <code>pip install cupy-cuda12x</code> <code>conda install -c conda-forge cupy cudatoolkit=12.1</code> CUDA 11.x <code>pip install cupy-cuda11x</code> <code>conda install -c conda-forge cupy cudatoolkit=11.8</code> CUDA 10.x <code>pip install cupy-cuda102</code> <code>conda install -c conda-forge cupy cudatoolkit=10.2</code> <p>If you do not have CUDA installed (e.g., WSL without GPU passthrough), install <code>pip install cupy-cuda12x</code> inside a CUDA-enabled environment or fall back to CPU-only runs (NumPy).</p>"},{"location":"tools/cupy/#quick-install-script-bashwsl","title":"Quick install script (Bash/WSL)","text":"<pre><code>#!/usr/bin/env bash\nset -euo pipefail\nCUDA_VER=${1:-12x}          # pass 11x or 12x depending on driver\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install --upgrade pip\npip install cupy-cuda${CUDA_VER} numpy tqdm matplotlib\npython - &lt;&lt;'PY'\nimport cupy as cp\ninfo = cp.cuda.runtime.getDeviceProperties(0)\nprint(\"CuPy OK on:\", info[\"name\"].decode())\nPY\n</code></pre> <p>Save as <code>scripts/install_cupy.sh</code>, run <code>bash scripts/install_cupy.sh 12x</code>, and you will end up with a virtual environment that already validates CuPy + GPU visibility.</p>"},{"location":"tools/cupy/#smoke-tests","title":"Smoke Tests","text":"<ol> <li>Driver check: <code>nvidia-smi</code> should report utilization and driver versions.</li> <li>Minimal CuPy test:</li> </ol> <pre><code>python - &lt;&lt;'PY'\nimport cupy as cp\na = cp.arange(1_000_000, dtype=cp.float32)\nb = cp.sin(a)\nprint(\"Device:\", cp.cuda.runtime.getDeviceProperties(0)[\"name\"].decode())\nprint(\"Result checksum:\", float(b.sum()))\nPY\n</code></pre> <p>If this succeeds, the CUDA runtime and CuPy wheel match. 3. Project stress test: <code>python scripts/test_cupy.py --stress-seconds 10 --stress-size 4096</code> pounds on GEMMs, reductions, and random kernels. Increase <code>--stress-seconds</code> if you suspect thermal throttling or unstable overclocks.</p>"},{"location":"tools/cupy/#using-the-backend-flags","title":"Using the Backend Flags","text":"<ul> <li>Scalar/vectorized MLPs remain CPU-only; the flag only affects vectorized/CNN entry points.</li> <li><code>python convolutional/main.py resnet18 --epochs 1 --batch-size 64 --gpu</code> calls <code>backend.use_gpu()</code> and every module switches to CuPy.</li> <li>Turn GPUs off by simply omitting <code>--gpu</code> or running <code>python -c \"from common import backend; backend.use_cpu()\"</code> before importing trainers.</li> </ul> <p>If <code>--gpu</code> raises <code>ModuleNotFoundError: cupy</code>, follow the installation steps above, then rerun the training command.</p>"},{"location":"tools/matplotlib/","title":"Matplotlib","text":""},{"location":"tools/matplotlib/#why-you-need-matplotlib","title":"Why You Need Matplotlib","text":"<p>Training is more insightful when you can visualize learning curves and misclassified samples. Matplotlib powers all figures produced by the trainers.</p>"},{"location":"tools/matplotlib/#where-it-appears-in-the-project","title":"Where It Appears in the Project","text":"<ul> <li>Loss curves: <code>scalar/main.py</code>, <code>vectorized/main.py</code>, and <code>convolutional/main.py</code> each plot <code>trainer.loss_history</code> after training.</li> <li>Misclassification grids: The main scripts gather misclassification data from the trainers and render grids directly.</li> <li>Sample visualization: <code>scalar/main.py</code> calls <code>DataUtility.sample_images(...)</code> and handles plotting itself.</li> </ul>"},{"location":"tools/matplotlib/#tips","title":"Tips","text":"<ul> <li>Use a backend that supports interactive windows (or run inside environments like Jupyter) if you want to inspect plots during training.</li> <li>If you\u2019re on a headless server, set <code>matplotlib.use(\"Agg\")</code> or save figures to disk instead of showing them.</li> </ul> <p>Official docs: https://matplotlib.org/stable/contents.html</p>"},{"location":"tools/numpy/","title":"Numpy","text":""},{"location":"tools/numpy/#why-you-need-numpy","title":"Why You Need NumPy","text":"<p>NumPy is the numerical backbone of this sandbox. Even when you run on GPU, the API surface mirrors NumPy semantics, so understanding it is essential.</p>"},{"location":"tools/numpy/#where-it-appears-in-the-project","title":"Where It Appears in the Project","text":"<ul> <li>Scalar/vectorized MLPs: All tensor math in <code>scalar/</code> and <code>vectorized/</code> ultimately uses NumPy arrays (or compatible wrappers).</li> <li>Common utilities: <code>common/data_utils.py</code>, <code>common/cross_entropy.py</code>, and <code>common/softmax.py</code> operate on <code>xp</code> objects, which default to NumPy unless you request GPU mode.</li> <li>Training scripts: When you run without <code>--gpu</code>, every forward/backward pass is powered by NumPy.</li> </ul>"},{"location":"tools/numpy/#skills-to-practice","title":"Skills to Practice","text":"<ul> <li>Creating arrays: <code>np.array</code>, <code>np.zeros</code>, <code>np.random.randn</code>.</li> <li>Vectorized ops: broadcasting, <code>np.dot</code>, <code>np.matmul</code>.</li> <li>Reshaping: <code>reshape</code>, <code>transpose</code>, <code>swapaxes</code>.</li> <li>Reductions: <code>np.sum</code>, <code>np.mean</code>, <code>np.max</code>, <code>np.argmax</code>.</li> </ul> <p>Brush up on NumPy and you\u2019ll understand 100% of the CPU path in this repo. The official docs are excellent: https://numpy.org/doc/stable/</p>"},{"location":"tools/tqdm/","title":"TQDM","text":""},{"location":"tools/tqdm/#why-you-need-tqdm","title":"Why You Need tqdm","text":"<p>Training large datasets is more manageable when you can see progress. <code>tqdm</code> wraps iterables with a live progress bar so you can monitor batch throughput, ETA, and overall health of a run.</p>"},{"location":"tools/tqdm/#where-it-appears-in-the-project","title":"Where It Appears in the Project","text":"<ul> <li>Trainers: Both <code>vectorized/trainer.py</code> and <code>convolutional/trainer.py</code> wrap batch loops inside <code>tqdm(range(...))</code>, yielding a progress bar labeled by epoch.</li> <li>Scripts: Any custom experiment you write can adopt the same pattern\u2014<code>for batch in tqdm(loader, desc=\"Epoch 1\")</code>.</li> </ul>"},{"location":"tools/tqdm/#handy-patterns","title":"Handy Patterns","text":"<pre><code>from tqdm import tqdm\nfor idx in tqdm(range(0, len(data), batch_size), desc=\"Epoch 3\", unit=\"batch\"):\n    ...\n</code></pre> <ul> <li>Use <code>unit</code> (<code>\"batch\"</code>, <code>\"step\"</code>, etc.) to keep terminology consistent with training logs.</li> <li>Combine with <code>leave=False</code> if you want the bar to disappear after completion in notebooks.</li> </ul> <p>Official docs: https://tqdm.github.io/</p>"}]}