# Neural Networks From Scratch â€” Tutorial Hub

Welcome to the learning guide for the **Neural Networks From Scratch (NNFS)** sandbox. This tutorial series walks through the entire projectâ€”why it exists, how it is organized, and how to experiment with the different neural-network implementations. Use the navigation list below to jump to the relevant modules.

## How to Use This Tutorial

1. **Start with the Project Tour** to understand the repository layout and supporting tools.
2. **Study the Implementation Progression** (scalar â†’ vectorized â†’ GPU) to see how performance grows from simple loops to accelerated kernels.
3. **Follow the Experiment Playbook** to run the MLP and CNN training scripts, save checkpoints, and compare results.
4. **Review the Concept Notes** whenever you need a refresher on activations, losses, optimizers, and regularization tricks implemented here.
5. **Explore the Architecture Gallery** to learn how classic and modern CNNs (LeNet through ConvNeXt) are assembled from the provided building blocks.

## Tutorial Modules

| Module | Description |
| --- | --- |
| [01. Project Tour](project-tour.md) | Repository structure, shared utilities, datasets, and helper scripts. |
| [02. Implementations & Hardware](implementations-and-hardware.md) | Scalar vs. vectorized code paths, backend switching, and GPU diagnostics. |
| [03. Running Experiments](running-experiments.md) | Step-by-step training commands, checkpointing workflows, and suggested investigations. |
| [04. Core Concepts](core-concepts.md) | Educational notes on neurons, activations, normalization, loss functions, optimizers, and convolution tricks. |
| [05. Architecture Gallery](architecture-gallery.md) | Background on each CNN provided (LeNet, AlexNet, VGG16, ResNet18, EfficientNet-Lite0, ConvNeXt-Tiny) plus dataset considerations. |

> Tip: Each module is self-contained. You can read them sequentially or jump directly to the section that matches your current work in the codebase.

Happy experimenting! ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ’»
