---
title: Optimizers
---

# Optimizer Hub



This hub links to dedicated tutorials for every optimization technique used in the sandbox. Each page covers the math, implementation details, CLI integration, and original references so you can understand *why* and *how* to tune training dynamics.

## Available Tutorials

| Topic | Highlights | Link |
| --- | --- | --- |
| Stochastic Gradient Descent | Vanilla SGD, momentum, Nesterov updates, weight decay | [Learn SGD](../optimizers/sgd.md) |
| Adam | Adaptive moments, bias correction, default hyperparameters | [Learn Adam](../optimizers/adam.md) |
| Lookahead | Slow/fast weight coupling, CLI flags (`--lookahead`) | [Learn Lookahead](../optimizers/lookahead.md) |
| Gradient Clipping | Why/when to clip, using `--grad-clip`, mathematical reasoning | [Learn gradient clipping](../optimizers/gradient-clipping.md) |

Use these references when you need to justify optimizer choices in lab reports or debug training behavior.

[Previous (Normalization Layers)](normalization.md) | [Back to Core Concepts](../core-concepts.md) | [Next (Regularization & Augmentation)](regularization.md)

[Back to Core Concepts](../core-concepts.md)

---

